[["index.html", "PSYCH 490.002 Spring 2023 notes About", " PSYCH 490.002 Spring 2023 notes Rick O. Gilmore, Ph.D. 2023-03-02 About These lecture notes are for your use as a student in PSYCH 490.002. "],["course-intro.html", "Course intro Prelude Today’s Topics Introductions Course overview Resources Themes/topics Structure Assignments &amp; evaluation Are we (scientists, the public) fooling ourselves? How can we know? Why might it matter? Learn more Next time…", " Course intro Prelude I like to have a series of songs or videos ready to play before class begins. They are loosely related to some of the themes of the course that day. Most are songs or artists I like. Showing them is just for fun. Today’s Topics Introductions Course overview Are we (scientists, the public) fooling ourselves? How can we know? Why might it matter? Introductions Teaching Assistant Garrett Thomas. M.S. gat84 AT-SIGN psu PERIOD edu Professor Rick O. Gilmore, Ph.D. Professor of Psychology Figure 1: https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/DenverCP.JPG/266px-DenverCP.JPG Figure 2: http://watson.brown.edu/ds/sites/all/themes/ds/img/header/brown_large.png Figure 3: https://www.wheretraveler.com/sites/default/files/styles/wt17_promoted/public/WashingtonDC-shutterstock_93633676.jpg?itok=IT7CL9PU Figure 4: https://ai.cs.cmu.edu/sites/default/files/CMU.png Figure 5: http://onwardstate.com/wp-content/uploads/2014/10/93 Figure 6: https://imaging.psu.edu Figure 7: https://nyu.databrary.org/web/images/logo/databrary-nav.svg Course overview Resources Themes/topics Structure Assignments/evaluation Resources Websites Syllabus: https://psu-psychology.github.io/psych-490-reproducibility-2023-spring/ Class notes: https://psu-psychology.github.io/psych-490-reproducibility-2023-spring-notes/ Book Other readings Book selections Scanned PDFs of book selections are available on Canvas: https://psu.instructure.com/courses/2245007/files/folder/readings Articles Retrieve them yourself via the URL (uniform resource locator) and the DOI (digital object identifier). Why do I do this? Themes/topics What is science trying to do? What practices and norms constitute better science? What practices and norms constitute poorer science? Is there a crisis of reproducibility or replicability in psychological science? Is there a crisis in other areas of science? What are scientists doing to address these criticisms? Structure Meet twice weekly Discussion/work sessions Do your homework; I will call on you. Assignments &amp; evaluation Class attendance Exercises Final project Are we (scientists, the public) fooling ourselves? How can we know? Why might it matter? A humorous perspective (NYU Health Sciences Library, 2013) Feynmann on ‘Cargo Cult Science’ Richard Feynman Figure 8: Richard P. Feynman, Wikipedia Who was he? Figure 9: (Feynman, 1974) What does Feynman mean by ‘Cargo Cult Science’? I think the educational and psychological studies I mentioned are examples of what I would like to call Cargo Cult Science. In the South Seas there is a Cargo Cult of people. During the war they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they’ve arranged to make things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas—he’s the controller—and they wait for the airplanes to land. They’re doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn’t work. No airplanes land. So I call these things Cargo Cult Science, because they follow all the apparent precepts and forms of scientific investigation, but they’re missing something essential, because the planes don’t land. More about “cargo cults”: (rjlipton, 2023) Implicit rules (practices or norms) in science …That is the idea that we all hope you have learned in studying science in school—we never explicitly say what this is, but just hope that you catch on by all the examples of scientific investigation. It is interesting, therefore, to bring it out now and speak of it explicitly. It’s a kind of scientific integrity, a principle of scientific thought that corresponds to a kind of utter honesty—a kind of leaning over backwards. For example, if you’re doing an experiment, you should report everything that you think might make it invalid—not only what you think is right about it: other causes that could possibly explain your results; and things you thought of that you’ve eliminated by some other experiment, and how they worked—to make sure the other fellow can tell they have been eliminated. Principle 1: Don’t fool yourself The first principle is that you must not fool yourself—and you are the easiest person to fool. So you have to be very careful about that. After you’ve not fooled yourself, it’s easy not to fool other scientists. You just have to be honest in a conventional way after that. Principle 2: Show how your maybe wrong I’m talking about a specific, extra type of integrity that is not lying, but bending over backwards to show how you’re maybe wrong, that you ought to do when acting as a scientist. And this is our responsibility as scientists, certainly to other scientists, and I think to laymen. Principle 3: Publish your results whichever way they come out One example of the principle is this: If you’ve made up your mind to test a theory, or you want to explain some idea, you should always decide to publish it whichever way it comes out. If we only publish results of a certain kind, we can make the argument look good. We must publish both kinds of result. Figure 10: (Oreskes, 2019) Flaws in how science is actually practiced Other kinds of errors are more characteristic of poor science. When I was at Cornell. I often talked to the people in the psychology department. One of the students told me she wanted to do an experiment that went something like this—I don’t remember it in detail, but it had been found by others that under certain circumstances, X, rats did something, A. She was curious as to whether, if she changed the circumstances to Y, they would still do, A. So her proposal was to do the experiment under circumstances Y and see if they still did A. I explained to her that it was necessary first to repeat in her laboratory the experiment of the other person—to do it under condition X to see if she could also get result A—and then change to Y and see if A changed. Then she would know that the real difference was the thing she thought she had under control. She was very delighted with this new idea, and went to her professor. And his reply was, no, you cannot do that, because the experiment has already been done and you would be wasting time. This was in about 1935 or so, and it seems to have been the general policy then to not try to repeat psychological experiments, but only to change the conditions and see what happens. Principle 4: Replicate then extend. Principle 5: Scientific integrity requires a form of freedom …So I have just one wish for you—the good luck to be somewhere where you are free to maintain the kind of integrity I have described, and where you do not feel forced by a need to maintain your position in the organization, or financial support, or so on, to lose your integrity. May you have that freedom. Questions to ponder Do you agree or disagree with Feynman’s characterizations of poor science? Why or why not? What are Feynman’s ‘rules’ or principles? Are these ‘rules’ or principles taught explicitly? Where and how? If not, why not? Do you agree or disagree that these rules are essential for scientific integrity? Why does Feynman suggest that you, the scientist or student, are the easiest one to fool? Going deeper Thu Jan 12: How science works or should work Tue Jan 17: Scientific norms and counter-norms Thu Jan 19: Adherence to norms and counter-norms. Exercise 01: Norms and counter-norms Begley’s ‘Bombshell’ Reading: (Harris, 2017), Chapter 1. Figure 11: (Harris, 2017) Background C. Glenn Begley (Begley &amp; Ellis, 2012) The scientific community assumes that the claims in a preclinical study can be taken at face value — that although there might be some errors in detail, the main message of the paper can be relied on and the data will, for the most part, stand the test of time. Unfortunately, this is not always the case. Over the past decade, before pursuing a particular line of research, scientists (including C.G.B.) in the haematology and oncology department at the biotechnology firm Amgen in Thousand Oaks, California, tried to confirm published findings related to that work. Fifty-three papers were deemed ‘landmark’ studies (see ‘Reproducibility of research findings’). It was acknowledged from the outset that some of the data might not hold up, because papers were deliberately selected that described something completely new, such as fresh approaches to targeting cancers or alternative clinical uses for existing therapeutics. Nevertheless, scientific findings were confirmed in only 6 (11%) cases. Even knowing the limitations of preclinical research, this was a shocking result. Journal Impact Factor \\(n\\) articles Mean number of citations for non-reproduced articles Mean number of citations of reproduced articles &gt;20 21 248 [3, 800] 231 [82-519] 5-19 32 168 [6, 1,909] 13 [3, 24] Table 1 from (Begley &amp; Ellis, 2012) Findings Findings of 6/53 published papers (11%) could be reproduced Original authors often could not reproduce their own work Earlier paper (Prinz, Schlange, &amp; Asadullah, 2011) had also found low rate of reproducibility. Paper titled “Believe it or not: How much can we rely on published data on potential drug targets?” Figure 1 from (Prinz et al., 2011) We received input from 23 scientists (heads of laboratories) and collected data from 67 projects, most of them (47) from the field of oncology. This analysis revealed that only in ∼20–25% of the projects were the relevant published data completely in line with our in-house findings (Prinz et al., 2011) Published papers (that can’t be reproduced) are cited hundreds or thousands of times Cost of irreproducible research estimated in billions of dollars (Freedman, Cockburn, &amp; Simcoe, 2015). An analysis of past studies indicates that the cumulative (total) prevalence of irreproducible preclinical research exceeds 50%, resulting in approximately US$28,000,000,000 (US$28B)/year spent on preclinical research that is not reproducible—in the United States alone. (Freedman et al., 2015). Figure 2 from (Freedman et al., 2015) Information about U.S. Research &amp; Development (R&amp;D) Expenditures from the Congressional Research Service. - Note that business accounts for 2-3x+ the Government’s share of R&amp;D expenditures. Questions to ponder Why does Harris call this a ‘bombshell’? Do you agree that it has/had or should have an ‘explosive’ impact? Why? Why do Begley &amp; Ellis focus on a journal’s impact factor? Why do Begley &amp; Ellis focus on citations to reproduced vs. non-reproduced articles? Why should non-scientists care? Why should scientists in other fields (not cancer biology) care? Going deeper Tues Jan 24: Replication crisis Thu Feb 2: Replication in cancer biology Tue Feb 7: Reproducibility and replicability reconsidered. Exercise 04: Replication Learn more Talk by Begley (CrossFit, 2019) Watching the talk by Begley is not required. But you might get inspired and decide to focus your final project around the topic. “What I’m alleging is that the reviewers, the editors of the so-called top-tier journals, grant review committees, promotion committees, and the scientific community repeatedly tolerate poor-quality science.” – C. Glenn Begley Next time… How science works (or should) Read (Ritchie, 2020), Chapter 1. (Nosek &amp; Bar-Anan, 2012) Optional (Sagan, 1996), Chapter 12, “The Fine Art of Baloney Detection” References Begley, C. G., &amp; Ellis, L. M. (2012). Drug development: Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a CrossFit. (2019, July). Dr. Glenn begley: Perverse incentives promote scientific laziness, exaggeration, and desperation. Youtube. Retrieved from https://www.youtube.com/watch?v=YJADzllTM9w Feynman, R. P. (1974). Cargo cult science. Retrieved from https://calteches.library.caltech.edu/51/2/CargoCult.htm Freedman, L. P., Cockburn, I. M., &amp; Simcoe, T. S. (2015). The economics of reproducibility in preclinical research. PLoS Biology, 13(6), e1002165. https://doi.org/10.1371/journal.pbio.1002165 Harris, R. (2017). Rigor mortis: How sloppy science creates worthless cures, crushes hope, and wastes billions (1st edition). Basic Books. Nosek, B. A., &amp; Bar-Anan, Y. (2012). Scientific utopia i: Opening scientific communication. Psychological Inquiry, 23(3), 217–243. https://doi.org/10.1080/1047840X.2012.692215 NYU Health Sciences Library. (2013, November). Data sharing and management snafu in 3 short acts (higher quality). Youtube. Retrieved from https://www.youtube.com/watch?v=66oNv_DJuPc Oreskes, N. (2019). Why trust science. Princeton University Press. Prinz, F., Schlange, T., &amp; Asadullah, K. (2011). Believe it or not: How much can we rely on published data on potential drug targets? Nature Reviews. Drug Discovery, 10(9), 712. https://doi.org/10.1038/nrd3439-c1 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 rjlipton. (2023, January). Cargo cult redo. https://rjlipton.wpcomstaging.com/2023/01/06/cargo-cult-redo/. Retrieved from https://rjlipton.wpcomstaging.com/2023/01/06/cargo-cult-redo/ Sagan, C. (1996). The demon-haunted world: Science as a candle in the dark (pp. 200–218). Ballantine Books. "],["how-science-works-or-should.html", "How science works (or should) Prelude Announcement Roadmap Newsflash Last time Today’s topics Discuss (Ritchie, 2020), Chapter 1 Discuss (Nosek &amp; Bar-Anan, 2012) Common themes Learn more Next time…", " How science works (or should) Prelude Announcement Penn State School of Theatre Free tickets Figure 12: https://theatre.psu.edu/centrestage Roadmap Newsflash Last time How science works (or should) Readings (Ritchie, 2020), Chapter 1. (Nosek &amp; Bar-Anan, 2012) Optional (Sagan, 1996), Chapter 12, “The Fine Art of Baloney Detection” Newsflash Hi Rick, Join us tomorrow at noon ET for a live presentation by the data science leaders at Roche about why they’re making open source the default for clinical trials in 2023. The presentation will take place on YouTube Live. You can tune in using this link. You can also add the event to your calendar as a reminder here. We couldn’t be more excited to highlight this historic industry shift. Thomas Neitmann, Ning Leng, and Dr. Kieran Martin will detail the years of preparation and innovation that went into making this shift a reality, along with how the organization plans to enable the transition of over 1,000 statistical programmers and statisticians to R and Python-centric tools. In the interest of providing additional context and drumming up some more excitement for the event, we highly recommend reading James Black’s ( Director of Insights Engineering at Roche) recent post on LinkedIn. We reserved time at the end of the presentation for questions, so don’t hesitate to show up curious! Additionally, if you have any questions that you’d like us to pass along to the team at Roche, feel free to respond directly to this email! From everyone at Posit and the incredible teams at Roche, we can’t wait to see you there! Robert @ RStudio Last time Comments on Feynman Comments on Begley &amp; Ellis Other comments or questions Extra credit opportunity Read (Feynman, 1974) or one of (Harris, 2017), Chapter 1, Begley’s Bombshell. PDF on Canvas or (Begley &amp; Ellis, 2012). In no more than one page, answer one of the questions posed in the notes from last time on Feynman or on Begley &amp; Ellis. Submit your answer via Canvas to Garrett by Monday, January 16, 2023 at 5:00 pm. Worth 2 points. Today’s topics How science works (or should) Discuss (Ritchie, 2020), Chapter 1 Discuss (Nosek &amp; Bar-Anan, 2012) Discuss (Ritchie, 2020), Chapter 1 Author Stuart J. Ritchie Scottish psychologist, Lecturer at King’s College London Figure 13: Stuart J. Ritchie Chapter 1: How Science Works “Science is a social construct.” How so? What are the consequences? What is the “process” of science? Figure 14: Figure 1 from (Munafò et al., 2017) Publish results But submit for peer review prior to publication Governed by “norms” Science’s social nature does come with weaknesses, however. Because scientists focus so much on trying to persuade their peers, which is the way they get those studies through peer review and oward to publication, it’s all too easy for them to disregard the real object of science: getting us closer to the truth. (Ritchie, 2020), Chapter 1, pp. 14-15. Discuss (Nosek &amp; Bar-Anan, 2012) Authors Brian Nosek Social Psychologist, Professor at University of Virginia Co-Founder, Center for Open Science (COS) Founder, Project Implicit Figure 15: Brian Nosek Yoav Bar-Anon Social Psychology, Professor at Tel Aviv University Scientific Utopia: I. Opening Scientific Communication What’s Utopia Where’s Part II? Nosek, B. A., Spies, J. R. &amp; Motyl, M. (2012). Scientific utopia II: Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058 Aims &amp; Claims Existing norms for scientific communication are rooted in anachronistic practices of bygone eras making them needlessly inefficient. We outline a path that moves away from the existing model of scientific communication to improve the efficiency in meeting the purpose of public science—knowledge accumulation. We call for six changes: (a) full embrace of digital communication; (b) open access to all published research; (c) disentangling publication from evaluation; (d) breaking the “one article, one journal” model with a grading system for evaluation and diversified dissemination outlets; (e) publishing peer review; and (f) allowing open, continuous peer review. We address conceptual and practical barriers to change and provide examples showing how the suggested practices are being used already. The critical barriers to change are not technical or financial; they are social. Although scientists guard the status quo, they also have the power to change it. Common themes Goals and purposes of science Utopian, idealistic aims Efficiency in achieving those goals High quality versus low quality science Ethics, scientific integrity Means of scientific communication Who has access, who should How are findings, articles, individuals evaluated Learn more Talk by Brian Nosek, (University of California Television (UCTV), 2018) Watching the talk by Nosek is not required. But he’s a very good speaker and an inspiring person. Next time… Scientific norms and counter-norms Read (Merton, 1973). (Mitroff, 1974). Assignment Complete (anonymous) survey on scientific norms and counter-norms. No write-up. References Begley, C. G., &amp; Ellis, L. M. (2012). Drug development: Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a Feynman, R. P. (1974). Cargo cult science. Retrieved from https://calteches.library.caltech.edu/51/2/CargoCult.htm Harris, R. (2017). Rigor mortis: How sloppy science creates worthless cures, crushes hope, and wastes billions (1st edition). Basic Books. Merton, R. W. (1973). The normative structure of science. In R. K. Merton &amp; N. W. Storer (Eds.), The sociology of science: Theoretical and empirical investigations (pp. 267–278). The University of Chicago Press. Mitroff, I. I. (1974). Norms and counter-norms in a select group of the apollo moon scientists: A case study of the ambivalence of scientists. American Sociological Review, 39(4), 579–595. https://doi.org/10.2307/2094423 Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Sert, N. P. du, … Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. https://doi.org/10.1038/s41562-016-0021 Nosek, B. A., &amp; Bar-Anan, Y. (2012). Scientific utopia i: Opening scientific communication. Psychological Inquiry, 23(3), 217–243. https://doi.org/10.1080/1047840X.2012.692215 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Sagan, C. (1996). The demon-haunted world: Science as a candle in the dark (pp. 200–218). Ballantine Books. University of California Television (UCTV). (2018, May). Improving openness and innovation in scholarly communication with brian nosek. Youtube. Retrieved from https://www.youtube.com/watch?v=YEwqohAjfZc "],["scientific-norms-and-counter-norms.html", "Scientific norms and counter-norms Roadmap Discuss (Merton, 1973) Discuss (Mitroff, 1974) Next time…", " Scientific norms and counter-norms Roadmap Last time How science works (or should) Readings (Merton, 1973). PDF on Canvas. (Mitroff, 1974). PDF on Canvas. Assignment Complete (anonymous) survey on scientific norms and counter-norms. Extra credit opportunity Read (Feynman, 1974) or one of (Harris, 2017), Chapter 1, Begley’s Bombshell. PDF on Canvas or (Begley &amp; Ellis, 2012). In no more than one page, answer one of the questions posed in the notes from last time on Feynman or on Begley &amp; Ellis. Submit your answer via Canvas to Garrett by Monday, January 16, 2023 at 5:00 pm. Worth 2 points. Discuss (Merton, 1973) Figure 16: Robert K. Merton: Wikipedia Note: Merton 1973 PDF on Canvas Difficult as the notion may appear to those reared in a culture that grants science a prominent if not a commanding place in the scheme of things, it is evident that science is not immune from attack, restraint, and repression. Writing a little while ago, Veblen could observe that the faith of western culture in science was unbounded, unquestioned, unrivaled. The revolt from science which then appeared so improbable as to concern only the timid academician who would ponder all contingencies, however remote, has now been forced upon the attention of scientist and layman alike. Local contagions of anti-intellectualism threaten to become epidemic. – (Merton, 1973), p. 267 Science is a deceptively inclusive word which refers to a variety of distinct though interrelated items. It is commonly used to denote (1) a set of characteristic methods by means of which knowledge is certified; (2) a stock of accumulated knowledge stemming from the application of these methods; (3) a set of cultural values and mores governing the activities termed scientific; or (4) any combination of the foregoing. – (Merton, 1973), p. 268 The ethos of science The mores of science possess a methodologic rationale but they are binding, not only because they are procedurally efficient, but because they are believed right and good. – (Merton, 1973), p. 270 Universalism See paragraph at top of p. 271 for context about the following: Yet this very deviation from the norm of universalism actually presupposed the the legitimacy of the norm. For nationalistic bias is oppropbrious only if judged in terms of the standard of universalism; within another institutional context, it is redefined as a virtue patriotism. Thus, in the process of condemning their violation, the mores are reaffirmed. – (Merton, 1973), p. 271 To restrict scientific careers on grounds other than lack of competence is to prejudice the furtherance of knowledge. – (Merton, 1973), p. 272 “Communism” Often called “communalism” today “Communism” in the nontechnical and extended sense of common ownership of goods…The substantive findings of science are a product of a social collaboration and are assigned to the community…Property rights in sciences are whittled down to a bare minimum by the rationale of the scientific ethic. – (Merton, 1973), p. 273 The institutional conception of science as part of the public domain is linked with the imperative for communication of scientific findings. Secrecy is the antithesis of this norm; full and open communication is its enactment. – (Merton, 1973), p. 274 Henry Cavendish was known for not publishing much of his work. Disinterestedness disinterested The virtual absence of fraud in the annals of science, which appears exceptional when compared with the record of other spheres of activity, has at times been attributed to the personal qualities of scientists…There is, in fact, no satisfactory evidence that such is the case; a more plausible explanation may be found in certain characteristcs of science itself…the activities of scientists are subject to rigorous policing, to a degree perhaps unparalleled in any other field of activity. – (Merton, 1973), p. 276 It is probable that the reputability of science and its lofty ethical status in the estimate of the layman is in no small measure due to technological achievements…the laity is often in no position to distinguish spurious from genuine claims…The presumably scientific pronouncements of totalitarian spokensmen are for the uninstructed laity of the same order as newspaper reports on an expanding universe or wave mechanics. – (Merton, 1973), p. 277 Organized Skepticism …is variously interrelated with other elements of the scientific ethos. It is both a methodological and institutional mandate. – (Merton, 1973), p. 277 The scientific investigator does not preserve the cleavage between the sacred and the profane, between that which requires uncritical respect and that which can be objectively analyzed. – (Merton, 1973), p. 277-278 Mnemonic: C-U-D-OS Discuss (Mitroff, 1974) Ian I. Mitroff Figure 17: Ian I. Mitroff: Fig source Amazon …the personal character of science infuses its entire structure. The testing and validating of scientific ideas is as governed by the deep personal character of science as the initial discovery of the ideas…Polyani (1958) argues that not only is this the case, but it ought to be the case. That is, science outght to be personal to its core. – (Mitroff, 1974), p. 580 …what the body of scientists thought about their fellow scientists. Who were perceived as most committed to their pet hypotheses? What did they think of such behavior? What did the scientists think of the abstract idea of commitment [to a pet hypothesis] itself… – (Mitroff, 1974), p. 582 All the interviews exhibit high affective content. They document the often fierce, sometimes bitter, competitive races for discovery and the intense emotions which permeate the doing of science. – (Mitroff, 1974), p. 585 The term “commitment” was used in three distinct (but related) senses. The first expresssed the notion of intellectual commitment, that is that scientific observerations were theory-laden…The second sense expressed the notion of affective commitment…The third sense expressed the notion that the entire process of science demanded deep personal commitment. – (Mitroff, 1974), p. 586 Table 4: A Tentative List of Norms and Counternorms Norms Counternorms 1. Faith in the moral virtue of rationality (Barber, 1952). 1. Faith in the moral virtue of rationality and nonrationality (cf., Tart, 1972). 2. Emotional neutrality as an instrumental condition for the achievement of rationality (Barber, 1952). 2. Emotional commitment as an instrumental condition for the achievement of rationality. 3. Universalism: “The acceptance or rejection of claims entering the list of science is not to depend on the personal or social attributes of their protagonist; his race, nationality, religion, class and personal qualities are as such irrelevant. Objectivity precludes particularism …. The imperative of universalism is rooted deep in the impersonal character of science” (Merton, 1949:607). 3. Particularism: “The acceptance or rejection of claims entering the list of science is to a large extent a function of who makes the claim” (Boguslaw, 1968:59). The social and psychological characteristics of the scientist are important factors influencing how his work will be judged. The work of certain scientists will be given priority over that of others (Mitroff, 1974b). The imperative of particularism is rooted deep in the personal character of science (Merton, 1963a; Polanyi, 1958). 4. Communism: “Property rights are reduced to the absolute minimum of credit for priority of discovery” (Barber, 1952:130). “Secrecy is the antithesis’ of this norm; full and open communication [of scientific results] its enactment” (Merton, 1949:611). 4. Solitariness (or, “Miserism” [Boguslaw, 1968:59]): Property rights are expanded to include protective control over the disposition of one’s discoveries; secrecy thus becomes a necessary moral act (Mitroff, 1974b). 5. Disinterestedness: “Scientists are expected by their peers to achieve the self-interest they have in work–satisfaction and in prestige through serving the [scientific] community interest directly” (Barber, 1952:132). 5. Interestedness: Scientists are expected by their close colleagues to achieve the self-interest they have in work-satisfaction and in prestige through serving their special communities of interest, e.g., their invisible college (Boguslaw, 1968:59; Mitroff, 1974b) 6. Organized scepticism: “The scientist is obliged … to make public his criticisms of the work of others when he believes it to be in error … no scientist’s contribution to knowledge can be accepted without careful scrutiny, and that the scientist must doubt his own findjngs as well as those of others” (Storer, 1966: 79). 6. Organized dogmatism: “Each scientist should make certain that previous work by others on which he bases his work is sufficiently identified so that others can be held responsible for inadequacies while any possible credit accrues to oneself” (Boguslaw, 1968:59). The scientist must believe in his own findings with utter conviction while doubting those of others with all his worth (Mitroff, 1974b). Adapted from Table 4. (Mitroff, 1974), p. 592. Next time… Adherence to norms and counter-norms Read (Kardash &amp; Edwards, 2012). (Macfarlane &amp; Cheng, 2008). Skim (Anderson, Ronning, Devries, &amp; Martinson, 2010). (Kim &amp; Kim, 2018). Assignment Exercise 01: Norms and counter-norms write-up References Anderson, M. S., Ronning, E. A., Devries, R., &amp; Martinson, B. C. (2010). Extending the mertonian norms: Scientists’ subscription to norms of research. The Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095 Begley, C. G., &amp; Ellis, L. M. (2012). Drug development: Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a Feynman, R. P. (1974). Cargo cult science. Retrieved from https://calteches.library.caltech.edu/51/2/CargoCult.htm Harris, R. (2017). Rigor mortis: How sloppy science creates worthless cures, crushes hope, and wastes billions (1st edition). Basic Books. Kardash, C. M., &amp; Edwards, O. V. (2012). Thinking and behaving like scientists: Perceptions of undergraduate science interns and their faculty mentors. Instructional Science, 40(6), 875–899. https://doi.org/10.1007/s11251-011-9195-0 Kim, S. Y., &amp; Kim, Y. (2018). The ethos of science and its correlates: An empirical analysis of scientists’ endorsement of mertonian norms. Science, Technology and Society, 23(1), 1–24. https://doi.org/10.1177/0971721817744438 Macfarlane, B., &amp; Cheng, M. (2008). Communism, universalism and disinterestedness: Re-examining contemporary support among academics for merton’s scientific norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y Merton, R. W. (1973). The normative structure of science. In R. K. Merton &amp; N. W. Storer (Eds.), The sociology of science: Theoretical and empirical investigations (pp. 267–278). The University of Chicago Press. Mitroff, I. I. (1974). Norms and counter-norms in a select group of the apollo moon scientists: A case study of the ambivalence of scientists. American Sociological Review, 39(4), 579–595. https://doi.org/10.2307/2094423 "],["adherence-to-norms-and-counter-norms.html", "Adherence to norms and counter-norms Roadmap Discuss (Kardash &amp; Edwards, 2012) How do our results compare? Discuss (Macfarlane &amp; Cheng, 2008) Looking ahead…", " Adherence to norms and counter-norms Roadmap Last time Scientific norms and counter-norms Readings (Kardash &amp; Edwards, 2012). (Macfarlane &amp; Cheng, 2008). Skim (Anderson et al., 2010). (Kim &amp; Kim, 2018). Survey results Assignment Exercise 01: Norms and counter-norms write-up How to read a paper What type of paper is it? Empirical, theoretical, review, opinion, other? Read the abstract carefully for answers to the following questions: Who is the target audience? Why did the author(s) do this work? What question were they trying to answer or what view were they trying to argue for? What is the take-home message or main finding(s)? For empirical papers, Who were the participants? What were their characteristics? What was measured? When or how often were the measurements taken? Read the Methods section Look at the figures in the Results section Do the figures “tell the story” of the paper? Imagine how you would complete the following synthesis: “This paper asks X by measuring Y in Z. It found Q, which suggests R. The paper is flawed because of S.” Read the Introduction and/or the Discussion if you need or want to know more. Other tips Highlight/flag unfamiliar terms or acronyms, but don’t stop reading to look them up until later. Keep a reference manager program; grab papers and sift through them later. Discuss (Kardash &amp; Edwards, 2012) Kardash, C. M. &amp; Edwards, O. V. (2012). Thinking and behaving like scientists: Perceptions of undergraduate science interns and their faculty mentors. Instructional Science, 40(6), 875–899. https://doi.org/10.1007/s11251-011-9195-0 Figure 18: CarolAnne M. Kardash: Source https://www.dignitymemorial.com/obituaries/las-vegas-nv/carolanne-kardash-10908219 Abstract We examined undergraduate research experiences (UREs) participants’ and their faculty mentors’ beliefs about the professional practices and dispositions of research scientists. In Study 1, 63 science interns and their mentors rated Merton’s (J Legal Political Sociol, 1:115–126, 1942) norms and Mitroff’s (Am Sociol Rev, 39(August):579–595, 1974) counter-norms of scientific practice. Specifically, we investigated what practices they believed research scientists should subscribe to (or not), and what practices they believed actually characterized research scientists’ behavior in the real world. Regarding idealized practice, mentors rated the norms significantly higher than did interns; mentors and interns generally did not differ in subscription to the counter-norms. Regarding actual practice, mentors believed scientists’ behaviors reflected counter-norms more than norms. Mentors further noted discrepancies between practices that should represent and actually did represent scientists’ work. In Study 2, interns and mentors listed characteristics associated with “thinking” and “behaving” like scientists. Personal and professional dispositions were mentioned more than intellectual and research skills. Although there was considerable consensus between faculty and intern perceptions, findings also revealed discrepancies that could be addressed in UREs, thereby aiding undergraduates’ socialization into the culture of scientific practice. Suggestions are provided for broadening interns’ conceptions of both scientists and science. …uncovering science majors’ conceptions of one aspect of academic life in the sciences—namely, what it means to think and behave like a research scientist. Specifically, we compared science undergraduates’ and faculty mentors’ perceptions of the practices and personal characteristics of research scientists. – (Kardash &amp; Edwards, 2012) As students find themselves immersed in the day to day practices of researchers, what values, practices, and dispositions capture their attention, become most salient to them and provide the foundation for their beliefs about what constitutes the professional identity of research scientists? As important, to what extent are students’ perceptions of these values, practices, and dispositions congruent with the perceptions of the faculty mentors? – (Kardash &amp; Edwards, 2012) Constantinides (2001) provides the following descriptions of the norms (p. 63): Universalism requires that knowledge claims be subjected to pre-established, impersonal criteria. Communality dictates that research belongs to the community of scientists rather than the individual researcher. Disinterestedness prescribes disinterested scientific activity, which is enforced by the accountability of scientists to their peers. Organized skepticism requires that claims be critically scrutinized in terms of empirical and logical criteria. – (Kardash &amp; Edwards, 2012) Study 1 Figure 19: Figure 1 from (Kardash &amp; Edwards, 2012) Study 2 Link to Table 1 This study could be replicated in some form as a final project. How do our results compare? A student could extend this analysis or do additional analyses as a final project. Discuss (Macfarlane &amp; Cheng, 2008) Macfarlane, B. &amp; Cheng, M. (2008). Communism, universalism and disinterestedness: Re-examining contemporary support among academics for Merton’s scientific norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y Figure 20: Bruce Macfarlane: https://hk.linkedin.com/in/bruce-macfarlane-244582a1 Figure 21: Ming Cheng: https://scholar.google.com/citations?user=4o9OsswAAAAJ&amp;hl=en Abstract This paper re-examines the relevance of three academic norms to contemporary academic life – communism, universalism and disinterestedness – based on the work of Robert Merton. The results of a web-based survey elicited responses to a series of value statements and were analysed using the weighted average method and through cross-tabulation. Results indicate strong support for communism as an academic norm defined in relation to sharing research results and teaching materials as opposed to protecting intellectual copyright and withholding access. There is more limited support for universalism based on the belief that academic knowledge should transcend national, political, or religious boundaries. Disinterestedness, defined in terms of personal detachment from truth claims, is the least popular contemporary academic norm. Here, the impact of a performative culture is linked to the need for a large number of academics to align their research interests with funding opportunities. The paper concludes by considering the claims of an alternate set of contemporary academic norms including capitalism, particularism and interestedness. Figure 22: Fig 1: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/1 Figure 23: Fig 2: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/2 Figure 24: Fig 3: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/3 Figure 25: Fig 4: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/4 Figure 26: Fig 5: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/5 Figure 27: Fig 6: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/6 Figure 28: Fig 7: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/7 Figure 29: Fig 8: https://link.springer.com/article/10.1007/s10805-008-9055-y/figures/8 Conclusion The results of the survey do not necessarily represent a ‘shift’ in values as Merton’s norms were not based on empirical data. While this research sample was broadly representative of academic staff by gender, this does not necessarily imply that it is representative of the academic profession in all respects. However, contemporary performative pressures on academic life may be having an impact in shaping, or perhaps re-shaping, some Mertonian norms. This is particularly apparent in respect to the norm of disinterestedness where large numbers of academics pragmatically align their research interests with funding opportunities. This finding may be related to a more competitive market-based university environment apparent in the UK and elsewhere internationally where it has been argued that the canons of scientific inquiry have been compromised by commercial pressures (Bok 2003). Despite these pressures, the norm of communism, in particular, still attracts strong popular espoused support which crosses disciplinary fields. The balance of evidence from this survey, though, suggests that market-based and commercial pressures might be beginning to subvert the Mertonian ideal. However, respondents’ support of universalism and disinterestedness varies with their subjects. In general, respondents from applied sciences showed stronger support for these two norms than respondents from the other subject fields. – (Macfarlane &amp; Cheng, 2008) This paper would be slightly harder to reproduce as a final project, but something similar is possible. Looking ahead… Assignment Exercise 01: Norms and counter-norms write-up Due next Thursday, January 26. Replication crisis (or not) Read (Ritchie, 2020), Chapter 2. (Begley &amp; Ellis, 2012) (Optional) (Oreskes, 2019), Chapter 7, pp. 228-244. References Anderson, M. S., Ronning, E. A., Devries, R., &amp; Martinson, B. C. (2010). Extending the mertonian norms: Scientists’ subscription to norms of research. The Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095 Begley, C. G., &amp; Ellis, L. M. (2012). Drug development: Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a Kardash, C. M., &amp; Edwards, O. V. (2012). Thinking and behaving like scientists: Perceptions of undergraduate science interns and their faculty mentors. Instructional Science, 40(6), 875–899. https://doi.org/10.1007/s11251-011-9195-0 Kim, S. Y., &amp; Kim, Y. (2018). The ethos of science and its correlates: An empirical analysis of scientists’ endorsement of mertonian norms. Science, Technology and Society, 23(1), 1–24. https://doi.org/10.1177/0971721817744438 Macfarlane, B., &amp; Cheng, M. (2008). Communism, universalism and disinterestedness: Re-examining contemporary support among academics for merton’s scientific norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y Oreskes, N. (2019). Why trust science. Princeton University Press. Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 "],["a-replication-crisis.html", "A replication crisis Roadmap Replication crisis…or not Reproducibility in psychological science Reproducibility in pre-clinical cancer biology Looking ahead", " A replication crisis Roadmap Announcements Assignment Exercise 01: Norms and counter-norms write-up Last time… Scientific norms and counter-norms Readings (Kardash &amp; Edwards, 2012). (Macfarlane &amp; Cheng, 2008). Skim (Anderson et al., 2010). (Kim &amp; Kim, 2018). Survey results Today’s topics A replication crisis…or not Read (Ritchie, 2020), Chapter 2. PDF on Canvas. (Begley &amp; Ellis, 2012) (Optional) (Oreskes, 2019), Chapter 7, pp. 228-244 Replication crisis…or not What proportion of findings in the published scientific literature (in the fields you care about) are actually true? 100% 90% 70% 50% 30% How do we define what “actually true” means? How widespread is the problem? Figure 30: (M. Baker, 2016) Figure 31: (M. Baker, 2016) Figure 32: (M. Baker, 2016) These questions could form the basis of a final project where a student or students re-run the survey with a different sample. What do the terms mean? Replication refers to testing the reliability of a prior finding with different data. Robustness refers to testing the reliability of a prior finding using the same data and a different analysis strategy. Reproducibility refers to testing the reliability of a prior finding using the same data and the same analysis strategy (Natl. Acad. Sci. Eng. Med. 2019). Each of the three notions plays an important role in assessing credibility. – (Nosek et al., 2022) In principle, all reported evidence should be reproducible. If someone applies the same analysis to the same data, the same result should occur. Reproducibility tests can fail for two reasons. A process reproducibility failure occurs when the original analysis cannot be repeated because of the unavailability of data, code, information needed to recreate the code, or necessary software or tools. An outcome reproducibility failure occurs when the reanalysis obtains a different result than the one reported originally. This can occur because of an error in either the original or the reproduction study. – (Nosek et al., 2022) Different types of reproducibility? (Goodman, Fanelli, &amp; Ioannidis, 2016) Methods reproducibility Enough details about materials &amp; methods recorded (&amp; reported) Same results with same materials &amp; methods (Goodman et al., 2016) Figure 33: If you got hit by a bus, could your colleagues replicate and build on your work? What’s your project’s ‘bus number’? Results reproducibility Same results from independent study (Goodman et al., 2016) Inferential reproducibility Same inferences from one or more studies or reanalyses (Goodman et al., 2016) Reproducibility in psychological science Replication failure: The “Lady Macbeth Effect” Read and discuss this Thursday, January 26, 2023 Replication failure: Priming effect Read and discuss next Tuesday, January 31, 2023 (Artner et al., 2021) We investigated the reproducibility of the major statistical conclusions drawn in 46 articles published in 2012 in three APA journals. After having identified 232 key statistical claims, we tried to reproduce, for each claim, the test statistic, its degrees of freedom, and the corresponding p value, starting from the raw data that were provided by the authors and closely following the Method section in the article. Out of the 232 claims, we were able to successfully reproduce 163 (70%), 18 of which only by deviating from the article’s analytical description. Thirteen (7%) of the 185 claims deemed significant by the authors are no longer so. The reproduction successes were often the result of cumbersome and time-consuming trial-and-error work, suggesting that APA style reporting in conjunction with raw data makes numerical verification at least hard, if not impossible. – (Artner et al., 2021) (Collaboration, 2015) Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716–aac4716. https://doi.org/10.1126/science.aac4716 Read and discuss on Tuesday, March 21, 2023 (Camerer et al., 2018) Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T.-H., Huber, J., Johannesson, M., Kirchler, M., Nave, G., Nosek, B. A., Pfeiffer, T., Altmejd, A., Buttrick, N., Chan, T., Chen, Y., Forsell, E., Gampa, A., Heikensten, E., Hummer, L., Imai, T., … Wu, H. (2018). Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. Nature Human Behaviour, 1. https://doi.org/10.1038/s41562-018-0399-z Read and discuss on Tuesday, March 21, 2023 (Whitt, Miranda, &amp; Tullett, 2022) Whitt, C. M., Miranda, J. F. &amp; Tullett, A. M. (2022). History of Replication Failures in Psychology. In W. O’Donohue, A. Masuda &amp; S. Lilienfeld (Eds.), Avoiding Questionable Research Practices in Applied Psychology (pp. 73–97). Springer International Publishing. https://doi.org/10.1007/978-3-031-04968-2_4 Is psychology harder than physics? Reproducibility in pre-clinical cancer biology Reading: (Begley &amp; Ellis, 2012) Discuss replication in cancer biology on Thursday, February 2, 2023. Background C. Glenn Begley (Begley &amp; Ellis, 2012) The scientific community assumes that the claims in a preclinical study can be taken at face value — that although there might be some errors in detail, the main message of the paper can be relied on and the data will, for the most part, stand the test of time. Unfortunately, this is not always the case. Over the past decade, before pursuing a particular line of research, scientists (including C.G.B.) in the haematology and oncology department at the biotechnology firm Amgen in Thousand Oaks, California, tried to confirm published findings related to that work. Fifty-three papers were deemed ‘landmark’ studies (see ‘Reproducibility of research findings’). It was acknowledged from the outset that some of the data might not hold up, because papers were deliberately selected that described something completely new, such as fresh approaches to targeting cancers or alternative clinical uses for existing therapeutics. Nevertheless, scientific findings were confirmed in only 6 (11%) cases. Even knowing the limitations of preclinical research, this was a shocking result. Journal Impact Factor \\(n\\) articles Mean number of citations for non-reproduced articles Mean number of citations of reproduced articles &gt;20 21 248 [3, 800] 231 [82-519] 5-19 32 168 [6, 1,909] 13 [3, 24] Table 1 from (Begley &amp; Ellis, 2012) Findings Findings of 6/53 published papers (11%) could be reproduced Original authors often could not reproduce their own work Earlier paper (Prinz et al., 2011) had also found low rate of reproducibility. Paper titled “Believe it or not: How much can we rely on published data on potential drug targets?” Figure 1 from (Prinz et al., 2011) We received input from 23 scientists (heads of laboratories) and collected data from 67 projects, most of them (47) from the field of oncology. This analysis revealed that only in ∼20–25% of the projects were the relevant published data completely in line with our in-house findings (Prinz et al., 2011) Published papers (that can’t be reproduced) are cited hundreds or thousands of times Cost of irreproducible research estimated in billions of dollars (Freedman et al., 2015). An analysis of past studies indicates that the cumulative (total) prevalence of irreproducible preclinical research exceeds 50%, resulting in approximately US$28,000,000,000 (US$28B)/year spent on preclinical research that is not reproducible—in the United States alone. (Freedman et al., 2015). Figure 2 from (Freedman et al., 2015) Information about U.S. Research &amp; Development (R&amp;D) expenditures from the Congressional Research Service. Note that business accounts for 2-3x+ the Government’s share of R&amp;D expenditures. Questions to ponder Why do Begley &amp; Ellis focus on a journal’s impact factor? Why do Begley &amp; Ellis focus on citations to reproduced vs. non-reproduced articles? Why should non-scientists care? Why should scientists in other fields (not cancer biology) care? Learn more Talk by Begley (CrossFit, 2019) “What I’m alleging is that the reviewers, the editors of the so-called top-tier journals, grant review committees, promotion committees, and the scientific community repeatedly tolerate poor-quality science.” – C. Glenn Begley Watching the talk by Begley is not required. But you might get inspired and decide to focus your final project around the topic. (Nosek et al., 2022) Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B., Rohrer, J. M., Romero, F., Scheel, A. M., Scherer, L. D., Schönbrodt, F. D. &amp; Vazire, S. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73(2022), 719–748. https://doi.org/10.1146/annurev-psych-020821-114157 Replication—an important, uncommon, and misunderstood practice—is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress. (Peng &amp; Hicks, 2021) Peng, R. D. &amp; Hicks, S. C. (2021). Reproducible research: A retrospective. Annual Review of Public Health, 42, 79–93. https://doi.org/10.1146/annurev-publhealth-012420-105110 Advances in computing technology have spurred two extraordinary phenomena in science: large-scale and high-throughput data collection coupled with the creation and implementation of complex statistical algorithms for data analysis. These two phenomena have brought about tremendous advances in scientific discovery but have raised two serious concerns. The complexity of modern data analyses raises questions about the reproducibility of the analyses, meaning the ability of independent analysts to recreate the results claimed by the original authors using the original data and analysis techniques. Reproducibility is typically thwarted by a lack of availability of the original data and computer code. A more general concern is the replicability of scientific findings, which concerns the frequency with which scientific claims are confirmed by completely independent investigations. Although reproducibility and replicability are related, they focus on different aspects of scientific progress. In this review, we discuss the origins of reproducible research, characterize the current status of reproducibility in public health research, and connect reproducibility to current concerns about the replicability of scientific findings. Finally, we describe a path forward for improving both the reproducibility and replicability of public health research in the future. Reading and writing a commentary on either of these articles might be a good final project. Looking ahead A replication failure: The “Lady Macbeth Effect” Read (Zhong &amp; Liljenquist, 2006) (Earp, Everett, Madva, &amp; Hamlin, 2014) Due Exercise 01: Norms and counter-norms write-up References Anderson, M. S., Ronning, E. A., Devries, R., &amp; Martinson, B. C. (2010). Extending the mertonian norms: Scientists’ subscription to norms of research. The Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095 Artner, R., Verliefde, T., Steegen, S., Gomes, S., Traets, F., Tuerlinckx, F., &amp; Vanpaemel, W. (2021). The reproducibility of statistical results in psychological research: An investigation using unpublished raw data. Psychological Methods, 26(5), 527–546. https://doi.org/10.1037/met0000365 Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature News, 533(7604), 452. https://doi.org/10.1038/533452a Begley, C. G., &amp; Ellis, L. M. (2012). Drug development: Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T.-H., Huber, J., Johannesson, M., … Wu, H. (2018). Evaluating the replicability of social science experiments in nature and science between 2010 and 2015. Nature Human Behaviour, 1. https://doi.org/10.1038/s41562-018-0399-z Collaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716 CrossFit. (2019, July). Dr. Glenn begley: Perverse incentives promote scientific laziness, exaggeration, and desperation. Youtube. Retrieved from https://www.youtube.com/watch?v=YJADzllTM9w Earp, B. D., Everett, J. A. C., Madva, E. N., &amp; Hamlin, J. K. (2014). Out, damned spot: Can the “macbeth effect” be replicated? Basic and Applied Social Psychology, 36(1), 91–98. https://doi.org/10.1080/01973533.2013.856792 Freedman, L. P., Cockburn, I. M., &amp; Simcoe, T. S. (2015). The economics of reproducibility in preclinical research. PLoS Biology, 13(6), e1002165. https://doi.org/10.1371/journal.pbio.1002165 Goodman, S. N., Fanelli, D., &amp; Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12–341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 Kardash, C. M., &amp; Edwards, O. V. (2012). Thinking and behaving like scientists: Perceptions of undergraduate science interns and their faculty mentors. Instructional Science, 40(6), 875–899. https://doi.org/10.1007/s11251-011-9195-0 Kim, S. Y., &amp; Kim, Y. (2018). The ethos of science and its correlates: An empirical analysis of scientists’ endorsement of mertonian norms. Science, Technology and Society, 23(1), 1–24. https://doi.org/10.1177/0971721817744438 Macfarlane, B., &amp; Cheng, M. (2008). Communism, universalism and disinterestedness: Re-examining contemporary support among academics for merton’s scientific norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., … Vazire, S. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73(2022), 719–748. https://doi.org/10.1146/annurev-psych-020821-114157 Oreskes, N. (2019). Why trust science. Princeton University Press. Peng, R. D., &amp; Hicks, S. C. (2021). Reproducible research: A retrospective. Annual Review of Public Health, 42, 79–93. https://doi.org/10.1146/annurev-publhealth-012420-105110 Prinz, F., Schlange, T., &amp; Asadullah, K. (2011). Believe it or not: How much can we rely on published data on potential drug targets? Nature Reviews. Drug Discovery, 10(9), 712. https://doi.org/10.1038/nrd3439-c1 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Whitt, C. M., Miranda, J. F., &amp; Tullett, A. M. (2022). History of replication failures in psychology. In W. O’Donohue, A. Masuda, &amp; S. Lilienfeld (Eds.), Avoiding questionable research practices in applied psychology (pp. 73–97). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-04968-2\\_4 Zhong, C.-B., &amp; Liljenquist, K. (2006). Washing away your sins: Threatened morality and physical cleansing. Science, 313(5792), 1451–1452. https://doi.org/10.1126/science.1130726 "],["a-replication-failure-the-lady-macbeth-effect.html", "A replication failure: The “Lady Macbeth Effect” Roadmap (Zhong &amp; Liljenquist, 2006) (Earp et al., 2014) Side-by-side comparisons Next time…", " A replication failure: The “Lady Macbeth Effect” Roadmap Announcements Assignment Exercise 01: Norms and counter-norms write-up due today Last time Replication crisis (or not) (Ritchie, 2020), Chapter 2. PDF on Canvas. (Begley &amp; Ellis, 2012) (Optional) (Oreskes, 2019), Chapter 7, pp. 228-244. Today’s topic A replication failure: The “Lady Macbeth Effect” (Zhong &amp; Liljenquist, 2006). PDF on Canvas. Supplemental material PDF on Canvas (Earp et al., 2014). PDF on Canvas (Zhong &amp; Liljenquist, 2006) Zhong, C.-B. &amp; Liljenquist, K. (2006). Washing away your sins: Threatened morality and physical cleansing. Science, 313(5792), 1451–1452. https://doi.org/10.1126/science.1130726 How to read a paper Type of paper: empirical|theoretical|review|opinion|other Physical cleansing has been a focal element in religious ceremonies for thousands of years. The prevalence of this practice suggests a psychological association between bodily purity and moral purity. In three studies, we explored what we call the “Macbeth effect”—that is, a threat to one’s moral purity induces the need to cleanse oneself. This effect revealed itself through an increased mental accessibility of cleansing-related concepts, a greater desire for cleansing products, and a greater likelihood of taking antiseptic wipes. Furthermore, we showed that physical cleansing alleviates the upsetting consequences of unethical behavior and reduces threats to one’s moral self-image. Daily hygiene routines such as washing hands, as simple and benign as they might seem, can deliver a powerful antidote to threatened morality, enabling people to truly wash away their sins. From abstract Who is audience? What question was explored? “Macbeth effect” Threats to moral purity Need to cleanse Increased mental accessibility of cleansing-related products Greater desire for cleansing products Greater likelihood of taking antiseptic wipes Who were participants? What were the participants’ characteristics? What measurements were taken? How often? From methods section Not in main text Look in Supplemental material Study 1 Participants. Sixty undergraduate students at Northwestern University participated in this study. Design and Procedure. Participants were randomly assigned to the cells of a 2-level single factor (Recall: ethical vs. unethical), between-participants design. They were led to separate breakout rooms upon arrival and were told that the researcher was interested in studying the differences in memories associated with ethical or unethical behaviors. In the ethical condition, participants were asked to describe in detail an ethical thing that they had done in the past and to describe any feelings or emotions they experienced. In the unethical condition, they were asked to describe an unethical deed and any emotions they experienced. This manipulation was adapted from a recall task in previous research (1). After the recall, participants engaged in a seemingly unrelated word completion task, in which they filled in blank spaces within word fragments to convert them into meaningful words. There were six word fragments, three of them (i.e., W _ _ H, SH _ _ ER, and S _ _ P) could be completed as either cleansing-related (i.e., wash, shower, and soap) or unrelated words (e.g., wish, shaker, and step). We summed the number of cleansing-related word fragments participants completed to form a composite measure of mental accessibility to cleansing-related concepts and submitted this measure to a one-way ANOVA. This measure and analysis have been used in many previous studies on mental accessibility (2, 3). Study 2 Participants. Twenty-seven undergraduate students at Northwestern University participated in this study. Design and Procedure. Participants were randomly assigned to the cells of a 2-level single factor (Prime: ethical versus unethical), between-participants design. They were led to individual breakout rooms upon arrival and engaged in multiple seemingly unrelated tasks. Instead of using the behavior recall task as in Study 1, we used an implicit manipulation for the ethical vs. unethical prime. In this manipulation, participants hand copied a short story written in the first person. They were told that the researcher was interested in studying the association between handwriting and personality. Participants in the ethical prime condition hand-copied the following story about an honest office worker: Two years ago, when I was a junior partner at a prestigious law firm, I was coming up for promotion against another junior partner, Chris. For several months, Chris had been working on a major case for the city that would make or break his career at the firm. However, he could not locate a key zoning document, without which, it was unlikely that he would have sufficient evidence to successfully argue his case. Late one evening, as I was rummaging through a corner filing cabinet, I happened to come across the zoning document that Chris was in desperate need of. I pulled it from the cabinet and placed it without a note on Chris’ desk, knowing that he would be so relieved when he arrived to work the next morning. Those in the unethical prime condition hand-copied the same story except that this time the office worker in the story decided to hide the critical document and sabotage the career of his or her competitor (the last sentence was replaced with, “I pulled it from the cabinet and walked over to the office shredder, knowing that my promotion would now be secured”). After completing the hand-copying task, participants engaged in a marketing task and rated the desirability of various products on a seven-point scale (1 = completely undesirable, 7 = completely desirable). Some of the products were cleansing products, including Dove shower soap, Crest toothpaste, Windex cleaner, Lysol disinfectant, and Tide detergent; other products included Post-it Notes, Nantucket Nectars juice, Energizer batteries, Sony CD cases, and Snickers bars. The desirability rating served as the dependent measure because participants who have a need for bodily cleansing should express greater desire towards cleansing related products. None of the participants suspected the link between the manipulation and the product rating task. Study 3 Participants. Thirty-two undergraduate students at Northwestern University participated in this study. Design and Procedure. Participants were randomly assigned to the cells of a 2-level single factor (Recall: ethical vs. unethical), between-participants design. They were led to individual breakout rooms upon arrival and engaged in the same memory recall task (i.e., the ethical vs. unethical recall) as in Study 1. They were then approached individually by the experimenter during the break and asked whether they would like to have an antiseptic cleansing wipe or an American pencil as a free gift (both items were visible in the experimenter’s hands). They were told that those materials were left over from a previous study and the experimenter would like to give them away as free gifts. Their choice between the pencil and wipe served as the dependent variable. The gifts were tested to make sure that they were equally desirable on an independent sample with 15 undergraduate students. These participants went through a similar but non-moral recall task (unrelated to ethics or cleanliness) and were then asked to choose between the wipes and pencils. Results confirmed equal desirability: 53% took the wipe and 47% took the pencil. Study 4 Participants. Forty-five undergraduate students at Northwestern University participated in this study. Design and Procedure. Participants were randomly assigned to the cells of a 2-level single factor (Intervention: cleansed vs. not-cleansed), between-participants design. They were led to separate breakout rooms upon arrival and told that they were going to engage in a computer task and a paper task. Participants were first asked to describe an unethical deed from their past via a computer program – the same task as in Study 1. They were then randomly assigned to one of two conditions. In the cleansed condition, participants were told that the Research Protection Board had recommended that we provide participants with hand-wipes after using public computers, and they were given an antiseptic cleansing wipe to use at that point. Those in the not-cleansed condition, however, were simply told that they had finished the computer task and could move on to the paper-based task. After the cleansing manipulation, participants in both conditions were given a paper-and-pencil task in which they assessed their current emotional state, including disgust, happiness, amusement, guilt, embarrassment, regret, calm, shame, confidence, excitement, distress, and anger. Finally, right before the end of the experiment, participants were solicited to volunteer to participate in a research study. They were told that a graduate student was looking for volunteers to help with one of her dissertation studies. The participation would be unpaid because the graduate student had no financial support yet desperately needed more data to complete her dissertation. Presumably, after participants recalled an unethical behavior from their past, they would be motivated to offer help to compensate for their wrongdoings. In contrast, participants who had cleansed their hands before being solicited for help would be less motivated to volunteer because the sanitation wipes had already washed away their moral stains and restored a suitable moral self. There are details about some additional analyses on Study 4 we do not excerpt here. From main text We first determined whether a threat to moral purity increases the mental accessibility of cleansing-related words. We asked participants to recall in detail either an ethical or unethical deed from their past and to describe any feelings or emotions they experienced. Then they engaged in a word completion task in which they converted word fragments into meaningful words. Of the six word fragments, three (W _ _ H, SH __ ER, and S __ P) could be completed as cleansing-related words (wash, shower, and soap) or as unrelated words (e.g., wish, shaker, and step). Participants who recalled an unethical deed generated more cleansing- related words than those who recalled an ethical deed [F(1,58) = 4.26, P = 0.04], suggesting that unethical behavior enhances the accessibility of cleansing-related concepts (Table 1). Figure 34: Table 1 from (Zhong &amp; Liljenquist, 2006) Erratum In the Report Washing away your sins: threatened morality and physical cleansing, the SEM values for Study 1 were entered incorrectly in Table 1. For the effect of ethical recall, the value should be 0.188, not 1.88, and for the effect of unethical recall, the value should be 0.177, not 1.77. The authors gratefully acknowledge A. Brouwer, S.A. Koppes, L. Wolters, L.D.J. Kuijper, and C. Zonneveld for pointing out this error. Study 2 investigated whether an implicit threat to moral purity produces a psychological desire for cleansing, through expressed preferences for cleansing products. Participants were told that we were investi- gating the relationship between handwriting and personality and were asked to hand-copy a short story written in the first person. The story described either an ethical, selfless deed (helping a co-worker) or an unethical act (sabotaging a co-worker) (9). Participants then rated the desirability of various products from 1 (completely undesirable) to 7 (completely desirable). Cleansing products included Dove shower soap, Crest toothpaste, Windex cleaner, Lysol disinfectant, and Tide detergent; other products included Post-it Notes, Nantucket Nectars juice, Energizer batteries, Sony CD cases, and Snickers bars. As expected, copying the unethical story increased the desirability of cleansing products as compared to copying the ethical story [F(1,25) = 6.99, P = 0.01], with no differ- ences between conditions for the noncleans- ing products [F(1,25) = 0.02, P = 0.89] (Fig. 1). Figure 35: Figure 1 from (Zhong &amp; Liljenquist, 2006) We sought to replicate the results of Study 2 using behavioral measures, so our next study examined the likelihood oftaking an antiseptic cleansing wipe after recalling an ethical or unethical deed. Participants engaged in the same recall task as in Study 1 and were then offered a free gift and given a choice between an antiseptic wipe and a pencil (verified in a control condition to be equally attractive offerings). Those who recalled an unethical deed were more likely to take the antiseptic wipe (67%) than were those who recalled an ethical deed (33%) (\\(\\chi^2\\) = 4.57, p = 0.03) (Table 1). In Study 4, participants described an unethical deed from their past (the same recall task as in Study 1). Afterwards, they either cleansed their hands with an antiseptic wipe or not. Then they completed a survey regarding their current emotional state (9). After completing the survey, participants were asked if they would volunteer without pay for another research study to help out a desperate graduate stu- dent. Presumably, participants who had cleansed their hands before being solicited for help would be less motivated to volunteer because the sanitation wipes had already washed away their moral stains and restored a suitable moral self. As predicted, physical cleansing significantly reduced volunteerism: 74% of those in the not-cleansed condition offered help, whereas only 41% of participants who had a chance to cleanse their hands offered to help (\\(\\chi^2\\)=5.02, P = 0.025). Thus, the direct compensatory behavior (i.e., volunteering) dropped by almost 50% when participants had a chance to physically cleanse after recalling an unethical behavior. (Earp et al., 2014) Earp, B. D., Everett, J. A. C., Madva, E. N. &amp; Hamlin, J. K. (2014). Out, Damned Spot: Can the “Macbeth Effect” Be Replicated? Basic and Applied Social Psychology, 36(1), 91–98. https://doi.org/10.1080/01973533.2013.856792 PSU Libraries link Zhong and Liljenquist (2006) reported evidence of a “Macbeth Effect” in social psychology: a threat to people’s moral purity leads them to seek, literally, to cleanse themselves. In an attempt to build upon these findings, we conducted a series of direct replications of Study 2 from Z&amp;L’s seminal report. We used Z&amp;L’s original materials and methods, investigated samples that were more representative of the general population, investigated samples from different countries and cultures, and substantially increased the power of our statistical tests. Despite multiple good-faith efforts, however, we were unable to detect a “Macbeth Effect” in any of our experiments. We discuss these findings in the context of recent concerns about replicability in the field of experimental social psychology. From abstract Type of paper: empirical|theoretical|review|opinion|empirical (partial replication) What question was explored? Who were participants? What were the participants’ characteristics? What measurements were taken? How often? From Study 1 method Participants in this study were 153 undergraduate students enrolled at a university in the United Kingdom. Participants were invited to take part via e-mail messages sent to departmental mailing lists and received a chocolate bar in exchange for their time. In Part 1, participants were randomly assigned to one of two priming conditions: ethical or unethical. In Part 2, participants rated a number of consumer products for their desirability on a scale of 1 to 7. In an attempt to prevent participants’ drawing any connections between Parts 1 and 2, they were told that Parts 1 and 2 were two separate experiments. Just as in Study 2 from Zhong and Liljenquist’s (2006) original report, participants were told they were taking part in an investigation into handwriting and personality and were asked to hand-copy a short story written in the first person. In the “unethical” condition, the paragraph described an unethical deed from the first-person perspective, as follows: Two years ago, when I was a junior partner at a prestigious law firm, I was coming up for promotion against another junior partner, Chris. For several months, Chris had been working on a major case for the city that would make or break his career at the firm. However, he could not locate a key zoning document, without which, it was unlikely that he would have sufficient evidence to successfully argue his case. Late one evening, as I was rummaging through a corner filing cabinet, I happened to come across the zoning document that Chris was in desperate need of. I pulled it from the cabinet and walked over to the office shredder, knowing that my promotion would now be secured. In the “ethical” condition, the paragraph was exactly the same, except that the last sentence read: I pulled it from the cabinet and placed it without a note on Chris’ desk, knowing that he would be so relieved when he arrived to work the next morning. Participants were then told that they were taking part in research looking at consumer marketing and were asked to rate the desirability of various products from 1 (completely undesirable) to 7 (completely desirable) and to say how much they would be willing to pay (£) for each product. The 10 items used were the exact same original items from Zhong and Liljenquist’s study, in their original order, with four items adapted slightly for a British sample by replacing unfamiliar American brands with equivalent British brands. The items and their order were specifically as follows: Post-it notes, Dove shower soap, Colgate toothpaste [Crest toothpaste in the original], pressed fruit juice 2 [Nanucket Nectars juice in the original], Energizer batteries, Sony CD cases, Windex glass cleaner, Dettoll disinfectant [Lysol countertop disinfectant in the original], Snickers candy bar, and Surf laundry detergent [Tide laundry detergent in the original]. Upon completion of the consumer products survey, participants were given a chocolate bar to compensate for their time and were thanked for their participation. From Study 1 results Independent samples t tests revealed no significant difference of condition on desirability of consumer product, t (151) = .03, p = .97, 95% CI [–0.29, 0.30], with no significant difference in the mean desirability of the cleansing items between the moral condition (M = 3.09) and immoral condition (M = 3.08). Similarly, there was no significant difference in how much participants were willing to pay for the consumer products, t (151) = − .28, p = .78, 95% CI [–0.36, 0.27], with comparable means in both the ethical condition (M = 2.19) and the unethical condition (M = 2.24). Looking at individual items, there were no significant effects of condition on the desirability of—or willingness to pay for—any individual cleansing item. From Study 2 method Participants. One hundred fifty-six American participants (83 female, M age = 33), using the Mechanical Turk (MTurk) online interface, participated in exchange for $.30. MTurk is a website that facilitates payment for the completion of tasks posted by researchers. Participant samples recruited through this service have been shown to be more representative of the general population than are student samples, and are known to yield reliable data (Buhrmester, Kwang, &amp; Gosling, 2011). Eight participants were excluded from analyses for failure to complete the questionnaires. Materials and procedure. As in Study 1, participants were randomly assigned to one of two priming conditions: ethical or unethical. All participants subsequently rated a number of consumer products for their desirability on a scale of 1 to 7, and noted how much they would be willing to pay ($) for each. To adapt the original priming materials from Zhong and Liljenquist for use in an online medium, the passages about helping/sabotaging a coworker were presented on participants’ computer screens with all of their punctuation removed. Participants were asked to retype the passage (rather than rewrite it, by hand, as in the original studies), inserting simple punctuation marks such as full stops (periods), commas, and capitalization where appropriate; participants could not advance to the next screen without performing this task, and all participants completed the priming task successfully. Although this design adjustment involved a slight departure from the rewriting task used in Zhong and Liljenquist’s original Study 2, we reasoned that our online-friendly prime might actually be more effective than the original. This is because to determine which punctuation marks were needed, participants would presumably have to process the meaning of the passage, whereas to hand-copy a passage exactly as it is written one could work by simple rote. After participants completed this punctuation priming task, they were shown a screen in which they were told that they were now taking part in research looking at consumer marketing. They were asked to rate the desirability of various products from 1 (completely undesirable) to 7 (completely desirable) and to say how much they would be willing to pay ($) for each product. The 10 items presented were the original items from Zhong and Liljenquist’s study, with no adjustments made to brand names, and were presented in their original order: Post-it notes, Dove shower soap, Crest toothpaste, Nanucket Nectars juice, Energizer batteries, Sony CD cases, Windex glass cleaner, Lysol countertop disinfectant, Snickers candy bar, and Tide laundry detergent. After completing the consumer products rating task, participants were shown a screen that thanked them for their efforts and were then directed to a link for claiming their small monetary reward. From Study 2 results Independent samples t tests revealed no significant difference of condition on desirability of the cleansing items, t(146) = − .79, p = .43, 95% CI [–0.62, 0.27] with comparable means in both the ethical (M = 4.23) and unethical (M = 4.41) conditions. Similarly, there was no significant difference in how much participants were willing to pay for the cleansing items, t(146) = .17, p = .87, 95% CI [–0.50, 0.59], with comparable means for both the ethical (M = 3.50) and unethical conditions (M = 3.46). Analyses were conducted on all individual cleansing items, and revealed no effect of condition on any individual item, with one exception: Consistent with predictions, a significant difference between conditions was found for how much participants were willing to pay for toothpaste, F (1, 146) = 4.76, p = .03, 95% CI [2.36, 3.03], with participants willing to pay more for the toothpaste in the unethical condition (M = 2.69) than in the ethical condition (M = 2.42). From Study 3 method Participants. Two hundred eighty-six Indian participants (92 female, M age = 31) using the MTurk online interface participated in exchange for $.30. Seventeen participants were excluded from analyses for failure to complete the questionnaires. Materials and procedure. The procedure was identical to that used in Study 2. Just as in Study 1, however, consumer product brand names had to be adjusted to accommodate a non-U.S. sample. In this case, brand names were replaced with generic descriptions of each product. Accordingly, participants were asked to rate their preferences concerning: sticky notes, shower soap, toothpaste, pressed fruit juice, batteries, CD cases, glass cleaner, countertop disinfectant, a candy bar, and laundry detergent. From Study 3 results Independent samples t tests revealed no significant difference of condition on either desirability, t(260) = − 1.83, p = .07, 95% CI [–0.42, 0.02] or how much participants were willing to pay, t(260) = − .29, p = .78, 95% CI [–1.37, 1.02]. The marginal effect found for desirability of cleansing items (p = .07, see earlier) was actually in the opposite direction to what Zhong and Liljenquist found in their original research: Indian participants in the unethical priming condition desired cleansing items (marginally) less (M = 5.25) than participants in the ethical priming condition (M = 5.46). There was no effect of condition on any individual item. Side-by-side comparisons Zhong et al. Study 2 Earp et al. Study 1 Earp et al. Study 2 Earp et al. Study 3 \\(n\\) 27 153 156 286 Country US UK US India Age group Undergraduate students Undergraduate students Adults Adults Setting in-person in-person online online Desirability of cleaning product: Ethical story 3.751 3.09 4.23 not reported Desirability of cleaning product: Unethical story 4.92 3.08 4.41 not reported Paper citations 15043 1004 Thoughts Neither study shared the data. The study with a figure (Zhong et al.) did not show individual participants, just a mean value. Earp et al. did not report standard errors of the mean (SEM) or standard deviations. Zhong et al. sample sizes in Experiment 2 were very small. Notable that Zhong et al. facilitated the replication effort by Earp et al. Could the story frame used as the prime be more salient or meaningful to participants of some cultural or economic backgrounds, but not others? Were Zhong et al. studies 1, 3, and 4 replicated by someone else? It might be interesting to write up a case study that focuses on the history of a finding and its replications. Next time… Replication failure: Priming effect Read (Bargh, Chen, &amp; Burrows, 1996) (Doyen, Klein, Pichon, &amp; Cleeremans, 2012) References Bargh, J. A., Chen, M., &amp; Burrows, L. (1996). Automaticity of social behavior: Direct effects of trait construct and stereotype-activation on action. Journal of Personality and Social Psychology, 71(2), 230–244. https://doi.org/10.1037//0022-3514.71.2.230 Begley, C. G., &amp; Ellis, L. M. (2012). Drug development: Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a Doyen, S., Klein, O., Pichon, C.-L., &amp; Cleeremans, A. (2012). Behavioral priming: It’s all in the mind, but whose mind? PloS One, 7(1), e29081. https://doi.org/10.1371/journal.pone.0029081 Earp, B. D., Everett, J. A. C., Madva, E. N., &amp; Hamlin, J. K. (2014). Out, damned spot: Can the “macbeth effect” be replicated? Basic and Applied Social Psychology, 36(1), 91–98. https://doi.org/10.1080/01973533.2013.856792 Oreskes, N. (2019). Why trust science. Princeton University Press. Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Zhong, C.-B., &amp; Liljenquist, K. (2006). Washing away your sins: Threatened morality and physical cleansing. Science, 313(5792), 1451–1452. https://doi.org/10.1126/science.1130726 estimated from Figure 35↩︎ estimated from Figure 35↩︎ As of 2023-01-25 via https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C39&amp;q=Washing+Away+Your+Sins%3A+Threatened+Morality+and+Physical+Cleansing+Zhong&amp;btnG=↩︎ as of 2023-01-25 https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C39&amp;q=Out%2C+Damned+Spot%3A+Can+the+“Macbeth+Effect”+Be+Replicated%3F&amp;btnG=↩︎ "],["a-replication-failure-the-priming-effect.html", "A replication failure: The priming effect Roadmap Bargh et al. (1996) Doyen et al. (2012) Next time…", " A replication failure: The priming effect Roadmap Last time A replication failure: The “Lady Macbeth Effect” (Zhong &amp; Liljenquist, 2006). PDF on Canvas. Supplemental material PDF on Canvas (Earp et al., 2014). PDF on Canvas Today’s topic A replication failure: The priming effect (Bargh et al., 1996); PDF on Canvas (Doyen et al., 2012) Bargh et al. (1996) Access Abstract-only publicly available, PDF $14.95 Web (HTML) and PDF versions available via PSU Libraries. Abstract Previous research has shown that trait concepts and stereotypes become active automatically in the presence of relevant behavior or stereotyped-group features. Through the use of the same priming procedures as in previous impression formation research, Experiment 1 showed that participants whose concept of rudeness was primed interrupted the experimenter more quickly and frequently than did participants primed with polite-related stimuli. In Experiment 2, participants for whom an elderly stereotype was primed walked more slowly down the hallway when leaving the experiment than did control participants, consistent with the content of that stereotype. In Experiment 3, participants for whom the African American stereotype was primed subliminally reacted with more hostility to a vexatious request of the experimenter. Implications of this automatic behavior priming effect for self-fulfilling prophecies are discussed, as is whether social behavior is necessarily mediated by conscious choice processes. (PsycINFO Database Record (c) 2016 APA, all rights reserved) (Source: journal abstract) Empirical article Key ideas Trait concepts and stereotypes Priming, automatic activation, automatic behavior effect Three studies Experiment 1: “Prime” rudeness/polite, measure interruptions Experiment 2: Prime “elderly” stereotype, measure speed of walking Experiment 3: Prime stereotype of African Americans, measure “hostility” Participants or characteristics? Experiment 1 A total of 34 students at New York University who were enrolled in the Introductory Psychology course participated in the experiment in partial fulfillment of a course research requirement. On their arrival at the laboratory waiting room they were randomly assigned to one of the three priming conditions. ~ 11 students/priming group Task: “Scrambled Sentence Test” (Srull &amp; Wyer, 1979), presented to participants as a test of language ability. For each of 30 items, participants are to use the five words listed to construct a grammatically correct four-word sentence as quickly as possible. The five words presented in a given test item are in scrambled order, such as “he it hides finds instantly.” 30 words, 15 were “critical priming stimuli”: “Rude”: aggressively, bold, rude, bother, disturb, intrude, annoyingly, interrupt, audaciously, brazen, impolitely, infringe, obnoxious, aggravating, and bluntly “Polite”: respect, honor, considerate, appreciate, patiently, cordially, yield, polite, cautiously, courteous, graciously, sensitively, discreetly, behaved, and unobtrusively “Neutral”: exercising, flawlessly, occasionally, rapidly, gleefully, practiced, optimistically, successfully, normally, send, watches, encourages, gives, clears, and prepares “Our dependent measure was the amount of time the participant would wait until interrupting the conversation between experimenter and confederate and ask to be given the next experimental task.” Post-survey: “On the survey were six questions concerning whether the experiment was interesting and whether it was considered a valid educational experience. The last three items concerned the experimenter, whether he or she was on time, whether he or she explained the study and answered questions, and the critical item: “Was the experimenter courteous and polite to you?” This the participant responded to on a −3 to +3 scale that ranged from −3 (not at all) to +3 (very much so). This item served as our check for a potential alternative interpretation of our results, to be discussed below.” Our primary dependent variable was the number of seconds the participants waited before interrupting the experimenter. A one-way analysis of variance (ANOVA) of these data, with priming condition as the single factor, revealed a significant main effect, F(2, 33) = 5.76, p = .008. Participants in the rude priming condition interrupted significantly faster (M = 326 s) than did participants in the neutral (M = 519 s) or polite (M = 558 s) priming conditions. Within the significant main effect, simple t tests revealed that the rude prime condition mean was significantly shorter than each of the other two means (both p s &lt; .04), which were not reliably different from one another (t &lt; 1). Although this result supports our hypothesis that social interaction behavior can be primed, the time-to-interruption distribution varied considerably from normality. Fully 21 of the 34 participants did not interrupt at all in the 10 min available to them, so that the time variable suffered from a severe ceiling effect. Thus, we reanalyzed the data in terms of the percentage of participants in each priming condition who interrupted at all during the 10-min period. Figure 36: Figure 1: (Bargh et al., 1996) Experiments 2a and 2b Participants were instructed to work on a scrambled-sentence task as part of a language proficiency experiment. The scrambled-sentence task contained words relevant to the elderly stereotype in the elderly priming condition, but all references to slowness, which is a quality stereotypically associated with elderly people, were excluded. Priming words “Elderly” condition: worried, Florida, old, lonely, grey, selfishly, careful, sentimental, wise, stubborn, courteous, bingo, withdraw, forgetful, retired, wrinkle, rigid, traditional, bitter, obedient, conservative, knits, dependent, ancient, helpless, gullible, cautious, and alone Neutral condition: thirsty, clean, private, .. Experiments 2a and 2b had 30 participants/study. Waiting until the participant had gathered all of his or her belongings, the experimenter told the participant that the elevator was down the hall and thanked him or her for participating. Using a hidden stopwatch, a confederate of the experimenter, who was sitting in a chair apparently waiting to talk to a professor in a nearby office, recorded the amount of time in seconds that the participant spent walking a length of the corridor starting from the doorway of the experimental room and ending at a broad strip of silver carpet tape on the floor 9.75 m away. Participants in the elderly priming condition (M = 8.28 s) had a slower walking speed compared to participants in the neutral priming condition (M = 7.30 s), t (28) = 2.86, p &lt; .01, as predicted. Figure 37: Figure 2: (Bargh et al., 1996) Awareness check study The crucial factor in concluding that these results show automatic effects on behavior derives from the perceiver’s lack of awareness of the influence of the words. Previous research (see review in Bargh, 1992) has indicated that it is not whether the primes are presented supraliminally or subliminally, but whether the individual is aware of the potential influence of the prime that is critical; diametrically opposite effects on judgments are obtained if the participant is aware versus not aware of a possible influence by the priming stimuli (see Lombardi, Higgins, &amp; Bargh, 1987; Strack &amp; Hannover, 1996). We conducted a subsequent study to explicitly test whether the participants were aware of the potential influence of the scrambled-sentence task. Our conclusions in terms of automatic social behavior depend on the participants’ not being aware of this influence. Method Nineteen male and female undergraduate students at New York University participated in the experiment to partially fulfill course credit. On arrival at the laboratory waiting room, participants were randomly assigned to either the elderly stereotype priming condition or the neutral priming condition. Participants took part in the experiment one at a time. They were informed that the purpose of the study was to investigate language proficiency and that they would complete a scrambled-sentence task. Participants were randomly administered either the version of the task containing words relevant to the elderly stereotype or the neutral version containing no stereotype-relevant words. Immediately after completion of the task, participants were asked to complete a version of the contingency awareness funnel debriefing, modeled after Page (1969). This contingency awareness debriefing contained items concerning the purpose of the study, whether the participant had suspected that the purpose of the experiment was different from what the experimenter had explained, whether the words had any relation to each other, what possible ways the words could have influenced their behavior, whether the participants could predict the direction of an influence if the experimenter had intended one, what the words in the scrambled-sentence task could have related to (if anything), and if the participant had suspected or had noticed any relation between the scrambled-sentence task and the concept of age. Afterward, the experimenter explained the hypotheses to the participants and thanked them for their help. Results and discussion Inspection of the responses revealed that only 1 of the 19 participants showed any awareness of a relationship between the stimulus words and the elderly stereotype. However, even this participant could not predict in what form or direction their behavior might have been influenced had such an influence occurred. Thus, it appears safe to conclude that the effect of the elderly priming manipulation on walking speed occurred nonconsciously. Experiment 3 Not the focus of replication by (Doyen et al., 2012), so we ignore here. Doyen et al. (2012) Access Full (HTML/PDF) access available to public. Abstract The perspective that behavior is often driven by unconscious determinants has become widespread in social psychology. Bargh, Chen, and Burrows’ (1996) famous study, in which participants unwittingly exposed to the stereotype of age walked slower when exiting the laboratory, was instrumental in defining this perspective. Here, we present two experiments aimed at replicating the original study. Despite the use of automated timing methods and a larger sample, our first experiment failed to show priming. Our second experiment was aimed at manipulating the beliefs of the experimenters: Half were led to think that participants would walk slower when primed congruently, and the other half was led to expect the opposite. Strikingly, we obtained a walking speed effect, but only when experimenters believed participants would indeed walk slower. This suggests that both priming and experimenters’ expectations are instrumental in explaining the walking speed effect. Further, debriefing was suggestive of awareness of the primes. We conclude that unconscious behavioral priming is real, while real, involves mechanisms different from those typically assumed to cause the effect. Empirical replication of one study in (Bargh et al., 1996). Methods and results Experiment 1 120 undergrads in Brussels, Belgium Two priming conditions Full list of words: https://doi.org/10.1371/journal.pone.0029081.s001 Before reaching the exit, participants were called back by the experimenter who pretended that he/she had forgotten to administer a final task. The debriefing that followed relied on a funnel questionnaire [1], [18] assessing participants’ awareness of the manipulation on three levels: 1-Awareness of the prime was assessed by asking participants increasingly specific questions about the presence of primes in the scrambled sentences. One particular question was a four-alternative forced-choice task (4-AFC) in which participants were required to choose between four pictures representing four social categories that could have been used as primes (i.e.: athletic person, Arabic person, handicapped person and elderly). 2-Awareness of the primed behavior was assessed by inviting participants to indicate how much they thought their walking speed had increased or decreased relative to their regular walking speed (responses were provided using an on-screen slider along a scale ranging from 0 to 100, with 50 representing their regular walking speed). 3-Awareness of the link between the prime and the primed behavior was assessed directly by asking participants whether they had noticed any link between the scrambled sentences task and their walking speed as they had left the room. The debriefing was also used to probe suspicion regarding the purpose of the experiment by asking increasingly accurate questions such as: “Do you think this experiment is related to any topic in particular?”, “Do you think this experiment could be related with manipulating behavior?”. Walking speed: In this analysis, we used participants’ walking speed as they entered the experiment room, (i.e., before priming) as a covariate. The results show no significant difference between the Prime (M=6.27″ SD=2.15) and the No-Prime group (M=6.39″ SD=1.11) in the time necessary to walk along the hallway after the priming manipulation (F (1, 119)&lt;1, \\(\\eta^2\\)=.01). Awareness of the prime. No participant reported having noticed anything unusual about the scrambled sentences task. Four participants (6.66%) in the Prime condition reported that the sentences were related to the stereotype of old persons. We tested the distribution of forced choices for both conditions using a two independent sample chi-squared test: the Prime group chose the picture of the old person above chance level whereas the No prime group was equally likely to choose all four pictures (χ2 (1)=5.43, p=0.023). Awareness of the effect. We computed the deviation of the slider from the initial position. No significant difference was found between the Prime (M=1.7) and the No-Prime (M=2.68) groups (t (1, 119)&lt;1, d=0.022). Awareness of the link. 96% of participants reported that they could not establish a link between the scrambled sentences task and their subsequent behavior. No experimenter reported having entertained any specific expectation about participants’ behavior. Experiment 2 …experimenters’ expectations could act as an amplifier of the effect of the prime and thus promote the primed behavior. To test these possibilities, we conducted a second experiment in which we manipulated the experimenters’ expectations about primed participants’ behavior. 50 new participants Experimenters’ expectations. Experimenters’ expectations about primed participants’ behavior were manipulated. One half of the experimenters were told that the primed participant would walk slower as result of the prime (i.e.: “Slow” condition), the other half were told that the participants would walk faster (i.e.: “Fast” condition). Each individual experimenter tested 5 participants randomly assigned to the Prime or the No-Prime condition. Experimenters’ expectations were shaped through a one hour briefing and persuasion session prior to the first participant’s session. In addition, the first participant whom an experimenter tested was a confederate who had been covertly instructed to act in the manner expected expected by the experimenter. Crucially, participants’ condition (i.e.: Prime or No-Prime) was made salient to the experimenter. Figure 38: Figure 1: (Doyen et al., 2012) Figure 39: Figure 2: (Doyen et al., 2012) Figure 40: Figure 3: (Doyen et al., 2012) Discussion First, in Experiment 1, despite the use of a larger sample and an experimental procedure devoid of the limitations present in the original experiment, we were not able to replicate Bargh et al’s [1] automatic effect of priming on walking speed. This led us to assume that crucial factors in this paradigm had remained unidentified. Experiment 2 was aimed at exploring such factors. Second, in Experiment 2 we were indeed able to obtain the priming effect on walking speed for both subjective and objective timings. Crucially however, this was only possible by manipulating experimenters’ expectations in such a way that they would expect primed participants to walk slower. Our results, however, cannot be explained solely in terms of a pure self-fulfilling prophecy effect [14], as the primed participants did not walk faster when tested by an experimenter who believed they would walk faster. Therefore it seems that the primes alone are not sufficient and must be in line with environmental cues such as the experimenters’ behavior in order to elicit the effect on walking speed. This is also supported by the fact that contrary factors (i.e.: primes related to the concept of age in conjunction with an experimenter expecting to observer a faster walking speed) did not alter participants’ walking speed. Regarding the subjective timings, we obtained a reverse effect on walking speed (i.e.: participants walking faster). This effect can be explained by the error committed by the experimenters, most likely as a result of their induced expectations. If we had used only human-operated measurement devices, as in Bargh et al.’s [1] experiment 2a and 2b, we would thus have erroneously concluded in a reverse priming effect. This very important point suggests that one must be cautious about (1) the type of measurement used in behavioral priming experiments as well as (2) the experimenters expectations. Subtle differences are particularly prone to external influences and potential biases. Third, most of the participants primed with the scrambled sentences task were aware of the social category they had been primed with. This result shows that one must be cautious when using the scrambled sentences task as a priming method. One must take into account that our participants were performing the experiment as part of a psychology course, which could have led to higher suspicion towards the scrambled sentences task resulting in a higher degree of awareness of the primes. Additionally, those participants who actually exhibited a slower walking speed reported in good proportion being aware of that particular behavior. Whether automatic behavioral priming can occur without awareness thus remains unclear. As a matter of fact, participants’ awareness of the prime and of the primed behavior could have led them to exert better conscious control over the latter and therefore impair its expression. Replication notes Neither study shared data Plots could have shown individual participants’ data Videos of procedures would have been helpful Both papers showed evidence of “digging” deeper Next time… Replication in cancer biology Errington, Mathur, et al. (2021) Errington, Denis, Perfito, Iorns, &amp; Nosek (2021) References Bargh, J. A., Chen, M., &amp; Burrows, L. (1996). Automaticity of social behavior: Direct effects of trait construct and stereotype-activation on action. Journal of Personality and Social Psychology, 71(2), 230–244. https://doi.org/10.1037//0022-3514.71.2.230 Doyen, S., Klein, O., Pichon, C.-L., &amp; Cleeremans, A. (2012). Behavioral priming: It’s all in the mind, but whose mind? PloS One, 7(1), e29081. https://doi.org/10.1371/journal.pone.0029081 Earp, B. D., Everett, J. A. C., Madva, E. N., &amp; Hamlin, J. K. (2014). Out, damned spot: Can the “macbeth effect” be replicated? Basic and Applied Social Psychology, 36(1), 91–98. https://doi.org/10.1080/01973533.2013.856792 Errington, T. M., Denis, A., Perfito, N., Iorns, E., &amp; Nosek, B. A. (2021). Challenges for assessing replicability in preclinical cancer biology. eLife, 10, e67995. https://doi.org/10.7554/eLife.67995 Errington, T. M., Mathur, M., Soderberg, C. K., Denis, A., Perfito, N., Iorns, E., &amp; Nosek, B. A. (2021). Investigating the replicability of preclinical cancer biology. eLife, 10, e71601. https://doi.org/10.7554/eLife.71601 Zhong, C.-B., &amp; Liljenquist, K. (2006). Washing away your sins: Threatened morality and physical cleansing. Science, 313(5792), 1451–1452. https://doi.org/10.1126/science.1130726 "],["replication-in-cancer-biology.html", "Replication in cancer biology Roadmap Errington, Mathur, et al. (2021) Errington, Denis, et al. (2021) Replication notes Next time…", " Replication in cancer biology Roadmap Last time A replication failure: The priming effect Bargh et al. (1996) Doyen et al. (2012) Today’s topic Replication in cancer biology Errington, Mathur, et al. (2021) Errington, Denis, et al. (2021) Errington, Mathur, et al. (2021) Errington, T. M., Mathur, M., Soderberg, C. K., Denis, A., Perfito, N., Iorns, E. &amp; Nosek, B. A. (2021). Investigating the replicability of preclinical cancer biology. eLife, 10, e71601. https://doi.org/10.7554/eLife.71601 Access Available on eLife without restriction Abstract Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85% smaller than the median effect size in the original experiments, and 92% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability. – Errington, Denis, et al. (2021) Type of paper: Empirical (replication) Who/what was sample: \\(n=50\\) experiments from \\(n=23\\) papers with \\(n=158\\) effects Measures of replication Numerical results (difference = “positive” effect or no difference = “null” effect) Same direction as original Same direction and statistically significant Original effect size (ES) in confidence interval (CI) for replication Replication ES in original CI Figure 41: Fig. 6 from Errington, Denis, et al. (2021) “We used seven criteria to assess the replicability of 158 effects in a selection of 23 papers reporting the results of preclinical research in cancer biology. Across multiple criteria, the replications provided weaker evidence for the findings than the original papers. For original positive effects that were reported as numerical values, the median effect size for the replications was 0.43, which was 85% smaller than the median of the original effect sizes (2.96). And although 79% of the replication effects were in the same direction as the original finding (random would be 50%), 92% of replication effect sizes were smaller than the original (combining numeric and representative images).” – Errington, Denis, et al. (2021) “A single failure to replicate a finding does not render a verdict on its replicability or credibility. A failure to replicate could occur because the original finding was a false positive.” “A failure to replicate could also occur because the replication was a false negative. This can occur if the replication was underpowered or the design or execution was flawed. Such failures are uninteresting but important.” “Successfully replicating a finding also does not render a verdict on its credibility. Successful replication increases confidence that the finding is repeatable, but it is mute to its meaning and validity. For example, if the finding is a result of unrecognized confounding influences or invalid measures, then the interpretation may be wrong even if it is easily replicated. Also, the interpretation of a finding may be much more general than is justified by the evidence. The particular experimental paradigm may elicit highly replicable findings, but also apply only to very specific circumstances that are much more circumscribed than the interpretation.” “After conducting dozens of replications, we can declare definitive understanding of precisely zero of the original findings. That may seem a dispiriting conclusion from such an intense effort, but it is the reality of doing research. Original findings provided initial evidence, replications provide additional evidence. Sometimes the replications increased confidence in the original findings, sometimes they decreased confidence. In all cases, we now have more information than we had. In no cases, do we have all the information that we need. Science makes progress by progressively identifying error and reducing uncertainty. Replication actively confronts current understanding, sometimes with affirmation, other times signaling caution and a call to investigate further. In science, that’s progress.” Errington, Denis, et al. (2021) Errington, T. M., Denis, A., Perfito, N., Iorns, E. &amp; Nosek, B. A. (2021). Challenges for assessing replicability in preclinical cancer biology. eLife, 10, e67995. https://doi.org/10.7554/eLife.67995 Access Available on eLife without restriction Abstract “We conducted the Reproducibility Project: Cancer Biology to investigate the replicability of preclinical research in cancer biology. The initial aim of the project was to repeat 193 experiments from 53 high-impact papers, using an approach in which the experimental protocols and plans for data analysis had to be peer reviewed and accepted for publication before experimental work could begin. However, the various barriers and challenges we encountered while designing and conducting the experiments meant that we were only able to repeat 50 experiments from 23 papers. Here we report these barriers and challenges. First, many original papers failed to report key descriptive and inferential statistics: the data needed to compute effect sizes and conduct power analyses was publicly accessible for just 4 of 193 experiments. Moreover, despite contacting the authors of the original papers, we were unable to obtain these data for 68% of the experiments. Second, none of the 193 experiments were described in sufficient detail in the original paper to enable us to design protocols to repeat the experiments, so we had to seek clarifications from the original authors. While authors were extremely or very helpful for 41% of experiments, they were minimally helpful for 9% of experiments, and not at all helpful (or did not respond to us) for 32% of experiments. Third, once experimental work started, 67% of the peer-reviewed protocols required modifications to complete the research and just 41% of those modifications could be implemented. Cumulatively, these three factors limited the number of experiments that could be repeated. This experience draws attention to a basic and fundamental concern about replication – it is hard to assess whether reported findings are credible.” Paper type: commentary/opinion piece Sought to repeat 193 experiments from 53 papers; were only able to repeat 50 experiments from 23 papers Had hoped to “[use] an approach in which the experimental protocols and plans for data analysis had to be peer reviewed and accepted for publication before experimental work could begin”, e.g., a registered report. Barriers Failure to report statistics for effect size calculations Unavailable for 32% of studies even after contacting authors. Failures to describe protocols for replication Original authors not especially helpful Protocols required changes, only some changes could be implemented. “Science is a system for accumulating knowledge. The credibility of knowledge claims relies, in part, on the transparency and repeatability of the evidence used to support them. As a social system, science operates with norms and processes to facilitate the critical appraisal of claims, and transparency and skepticism are virtues endorsed by most scientists (Anderson et al., 2007). Science is also relatively non-hierarchical in that there are no official arbiters of the truth or falsity of claims. However, the interrogation of new claims and evidence by peers occurs continuously, and most formally in the peer review of manuscripts prior to publication. Once new claims are made public, other scientists may question, challenge, or extend them by trying to replicate the evidence or to conduct novel research. The evaluative processes of peer review and replication are the basis for believing that science is self-correcting. Self-correction is necessary because mistakes and false starts are expected when pushing the boundaries of knowledge. Science works because it efficiently identifies those false starts and redirects resources to new possibilities. We believe everything we wrote in the previous paragraph except for one word in the last sentence – efficiently. Science advances knowledge and is self-correcting, but we do not believe it is doing so very efficiently. Many parts of research could improve to accelerate discovery. In this paper, we report the challenges confronted during a large-scale effort to replicate findings in cancer biology, and describe how improving transparency and sharing can make it easier to assess rigor and replicability and, therefore, to increase research efficiency.” “Reproducibility refers to whether the reported findings are repeatable using the same analysis on the same data as the original study. Robustness refers to whether the reported findings are repeatable using reasonable alternative analysis strategies on the same data as the original study. Replicability refers to whether the reported findings are repeatable using new data (NAS, 2019).” Figure 42: Fig 1 from Errington, Denis, et al. (2021) Figure 43: Fig 1 (suppl) from Errington, Denis, et al. (2021) Figure 44: Fig 2 from Errington, Denis, et al. (2021) Figure 45: Fig 2 (suppl 2) from Errington, Denis, et al. (2021) Replication notes Pre-registered All data shared on Open Science Framework Most of the results can be gleaned from the figures Next time… The 4 (5?) Rs Read Nosek et al. (2022) Goodman et al. (2016) Skim Fidler &amp; Wilcox (2021) References Bargh, J. A., Chen, M., &amp; Burrows, L. (1996). Automaticity of social behavior: Direct effects of trait construct and stereotype-activation on action. Journal of Personality and Social Psychology, 71(2), 230–244. https://doi.org/10.1037//0022-3514.71.2.230 Doyen, S., Klein, O., Pichon, C.-L., &amp; Cleeremans, A. (2012). Behavioral priming: It’s all in the mind, but whose mind? PloS One, 7(1), e29081. https://doi.org/10.1371/journal.pone.0029081 Errington, T. M., Denis, A., Perfito, N., Iorns, E., &amp; Nosek, B. A. (2021). Challenges for assessing replicability in preclinical cancer biology. eLife, 10, e67995. https://doi.org/10.7554/eLife.67995 Errington, T. M., Mathur, M., Soderberg, C. K., Denis, A., Perfito, N., Iorns, E., &amp; Nosek, B. A. (2021). Investigating the replicability of preclinical cancer biology. eLife, 10, e71601. https://doi.org/10.7554/eLife.71601 Fidler, F., &amp; Wilcox, J. (2021). Reproducibility of scientific results. In E. N. Zalta (Ed.), The stanford encyclopedia of philosophy (Summer 2021). Metaphysics Research Lab, Stanford University. Retrieved from https://plato.stanford.edu/archives/sum2021/entries/scientific-reproducibility/ Goodman, S. N., Fanelli, D., &amp; Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12–341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., … Vazire, S. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73(2022), 719–748. https://doi.org/10.1146/annurev-psych-020821-114157 "],["the-5-rs.html", "The 5 R’s Roadmap Papers Next time", " The 5 R’s Roadmap The 4 (5?) R’s Nosek et al. (2022) Goodman et al. (2016) Skim Fidler &amp; Wilcox (2021) Papers Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B., Rohrer, J. M., Romero, F., Scheel, A. M., Scherer, L. D., Schönbrodt, F. D. &amp; Vazire, S. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73(2022), 719–748. https://doi.org/10.1146/annurev-psych-020821-114157 Goodman, S. N., Fanelli, D. &amp; Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12–ps341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 “The language and conceptual framework of ‘research reproducibility’ are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ‘truth’.” – Goodman et al. (2016) Figure 46: Figure 1: Goodman et al. (2016) Figure 47: Table 1 from Goodman et al. (2016) Reproducibility “Although the importance of multiple studies corroborating a given result is acknowledged in virtually all of the sciences (Fig. 1), the modern use of “reproducible research” was originally applied not to corroboration, but to transparency, with application in the computational sciences. Computer scientist Jon Claerbout coined the term and associated it with a software platform and set of procedures that permit the reader of a paper to see the entire processing trail from the raw data and code to figures and tables (4).” – Goodman et al. (2016) “According to a U.S. National Science Foundation (NSF) subcommittee on replicability in science (9), ‘reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results…. Reproducibility is a minimum necessary condition for a finding to be believable and informative.’” – Goodman et al. (2016) “…Reproducibility refers to testing the reliability of a prior finding using the same data and the same analysis strategy (Natl. Acad. Sci. Eng. Med. 2019)…In principle, all reported evidence should be reproducible. If someone applies the same analysis to the same data, the same result should occur…” – Nosek et al. (2022) Methods reproducibility Goodman et al. (2016) Enough details about materials &amp; methods recorded (&amp; reported), so… Same results with same materials &amp; methods Artner et al. (2021). “In the biomedical sciences, this means, at minimum, a detailed study protocol, a description of measurement procedures, the data gathered, the data used for analysis with descriptive metadata, the analysis software and code, and the final analytical results. In laboratory science, how key reagents and biological materials were created or obtained can be critical. In theory, these requirements are clear, but in practice, the level of procedural detail needed to describe a study as “methodologically reproducible” does not have consensus.” – Goodman et al. (2016) What’s your project’s ‘bus number’? Replicability Results reproducibility Goodman et al. (2016) Same results from “independent study whose procedures are as closely matched to the original experiment as possible.” Inferential reproducibility Goodman et al. (2016) Same inferences from one or more studies or reanalyses “Inferential reproducibility is not identical to results reproducibility or to methods reproducibility, because scientists might draw the same conclusions from different sets of studies and data or could draw different conclusions from the same original data, sometimes even if they agree on the analytical results.” Robustness “…Robustness refers to testing the reliability of a prior finding using the same data and a different analysis strategy…” – Nosek et al. (2022) “Many analysts”: (Botvinik-Nezer et al., 2020; Silberzahn et al., 2018) Two additional “R’s” Gennetian2022-us Figure 48: Table 1 from Gennetian2022-us Next time Fraud &amp; misconduct (Ritchie, 2020), Chapter 3. (Bhattacharjee, 2013) (Skim) (Levelt, Drenth, &amp; Noort, 2012) (Skim) (Carpenter, 2012) References Artner, R., Verliefde, T., Steegen, S., Gomes, S., Traets, F., Tuerlinckx, F., &amp; Vanpaemel, W. (2021). The reproducibility of statistical results in psychological research: An investigation using unpublished raw data. Psychological Methods, 26(5), 527–546. https://doi.org/10.1037/met0000365 Bhattacharjee, Y. (2013). The mind of a con man. The New York Times. Retrieved from https://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html Botvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., … Schonberg, T. (2020). Variability in the analysis of a single neuroimaging dataset by many teams. Nature, 582(7810), 84–88. https://doi.org/10.1038/s41586-020-2314-9 Carpenter, S. (2012). Harvard psychology researcher committed fraud, U.S. Investigation concludes. Science. https://doi.org/10.1126/article.26972 Fidler, F., &amp; Wilcox, J. (2021). Reproducibility of scientific results. In E. N. Zalta (Ed.), The stanford encyclopedia of philosophy (Summer 2021). Metaphysics Research Lab, Stanford University. Retrieved from https://plato.stanford.edu/archives/sum2021/entries/scientific-reproducibility/ Goodman, S. N., Fanelli, D., &amp; Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12–341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 Levelt, W. J. M., Drenth, P. J. D., &amp; Noort, E. (2012). Flawed science: The fraudulent research practices of social psychologist diederik stapel. https://pure.mpg.de/rest/items/item_1569964/component/file_1569966/content; pure.mpg.de. Retrieved from https://pure.mpg.de/rest/items/item_1569964/component/file_1569966/content Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., … Vazire, S. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73(2022), 719–748. https://doi.org/10.1146/annurev-psych-020821-114157 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., … Nosek, B. A. (2018). Many analysts, one data set: Making transparent how variations in analytic choices affect results. Advances in Methods and Practices in Psychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646 "],["fraud-misconduct.html", "Fraud &amp; misconduct Roadmap Stapels Hauser Next time", " Fraud &amp; misconduct Roadmap Fraud &amp; misconduct (Ritchie, 2020), Chapter 3. (Bhattacharjee, 2013) (Skim) (Levelt et al., 2012) (Skim) (Carpenter, 2012) Final project discussion Stapels Figure 49: Diedrik Stapels from Bhattacharjee (2013) Stapel did not deny that his deceit was driven by ambition. But it was more complicated than that, he told me. He insisted that he loved social psychology but had been frustrated by the messiness of experimental data, which rarely led to clear conclusions. His lifelong obsession with elegance and order, he said, led him to concoct sexy results that journals found attractive. “It was a quest for aesthetics, for beauty — instead of the truth,” he said. He described his behavior as an addiction that drove him to carry out acts of increasingly daring fraud, like a junkie seeking a bigger and better high. – Bhattacharjee (2013) In his early years of research — when he supposedly collected real experimental data — Stapel wrote papers laying out complicated and messy relationships between multiple variables. He soon realized that journal editors preferred simplicity. “They are actually telling you: ‘Leave out this stuff. Make it simpler,’” Stapel told me. Before long, he was striving to write elegant articles. On a Sunday morning, as we drove to a village near Maastricht to see his parents, Stapel reflected on why his behavior had sparked such outrage in the Netherlands. “People think of scientists as monks in a monastery looking out for the truth,” he said. “People have lost faith in the church, but they haven’t lost faith in science. My behavior shows that science is not holy.” What the public didn’t realize, he said, was that academic science, too, was becoming a business. “There are scarce resources, you need grants, you need money, there is competition,” he said. “Normal people go to the edge to get that money. Science is of course about discovery, about digging to discover the truth. But it is also communication, persuasion, marketing. I am a salesman. I am on the road. People are on the road with their talk. With the same talk. It’s like a circus.” He named two psychologists he admired — John Cacioppo and Daniel Gilbert — neither of whom has been accused of fraud. “They give a talk in Berlin, two days later they give the same talk in Amsterdam, then they go to London. They are traveling salesmen selling their story.” – Bhattacharjee (2013) The adjective “sloppy” seems charitable. Several psychologists I spoke to admitted that each of these more common practices was as deliberate as any of Stapel’s wholesale fabrications. Each was a choice made by the scientist every time he or she came to a fork in the road of experimental research — one way pointing to the truth, however dull and unsatisfying, and the other beckoning the researcher toward a rosier and more notable result that could be patently false or only partly true. What may be most troubling about the research culture the committees describe in their report are the plentiful opportunities and incentives for fraud. “The cookie jar was on the table without a lid” is how Stapel put it to me once. Those who suspect a colleague of fraud may be inclined to keep mum because of the potential costs of whistle-blowing. – Bhattacharjee (2013) Fraud like Stapel’s — brazen and careless in hindsight — might represent a lesser threat to the integrity of science than the massaging of data and selective reporting of experiments. The young professor who backed the two student whistle-blowers told me that tweaking results — like stopping data collection once the results confirm a hypothesis — is a common practice. “I could certainly see that if you do it in more subtle ways, it’s more difficult to detect,” Ap Dijksterhuis, one of the Netherlands’ best known psychologists, told me. He added that the field was making a sustained effort to remedy the problems that have been brought to light by Stapel’s fraud. – Bhattacharjee (2013) Hauser Former Harvard University psychologist Marc Hauser fabricated and falsified data and made false statements about experimental methods in six federally funded studies, according to a report released yesterday by the U.S. Department of Health and Human Services’s Office of Research Integrity (ORI). Hauser, who resigned from his Harvard faculty position in 2011 after an internal investigation found him responsible for research misconduct, wrote in a statement that although he has “fundamental differences” with some of the new report’s findings, “I acknowledge that I made mistakes.” He did not admit deliberate misconduct, however, and implied that his mistake was that he “tried to do too much” and “let important details get away from my control.” – Carpenter (2012) Note: link to report in article: http://www.ofr.gov/OFRUpload/OFRData/2012-21992_PI.pdf is broken. I went to https://ori.hhs.gov and searched for “Hauser”. I found this https://ori.hhs.gov/sites/default/files/september_vol20_no4.pdf. Next time Retraction and scientific integrity Read (Brainerd &amp; You, 2018) Explore https://retractionwatch.com/ https://ori.hhs.gov/ Discuss assignment Exercise 02: P-hack your way to scientific glory References Bhattacharjee, Y. (2013). The mind of a con man. The New York Times. Retrieved from https://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html Brainerd, J., &amp; You, J. (2018). What a massive database of retracted papers reveals about science publishing’s “death penalty.” Science. https://doi.org/10.1126/science.aav8384 Carpenter, S. (2012). Harvard psychology researcher committed fraud, U.S. Investigation concludes. Science. https://doi.org/10.1126/article.26972 Levelt, W. J. M., Drenth, P. J. D., &amp; Noort, E. (2012). Flawed science: The fraudulent research practices of social psychologist diederik stapel. https://pure.mpg.de/rest/items/item_1569964/component/file_1569966/content; pure.mpg.de. Retrieved from https://pure.mpg.de/rest/items/item_1569964/component/file_1569966/content Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 "],["retraction-and-scientific-integrity.html", "Retraction and scientific integrity Roadmap Brainerd &amp; You (2018) Discussion of Retraction Watch Discussion of Office of Research Integrity (ORI) https://ori.hhs.gov/ Next time…", " Retraction and scientific integrity Roadmap Retraction and scientific integrity Read Brainerd &amp; You (2018) Explore https://retractionwatch.com/ https://ori.hhs.gov/ Discuss assignment Exercise 02: P-hack your way to scientific glory Due next Tuesday, February 21, 2023. Brainerd &amp; You (2018) Study methodology. Figure 50: Brainerd &amp; You (2018) Then, in 2009, the Committee on Publication Ethics (COPE), a nonprofit group in Eastleigh, U.K., that now advises more than 12,000 journal editors and publishers, released a model policy for how journals should handle retractions. – Brainerd &amp; You (2018) Link to policy is broken In searching on COPE page, I found this: https://publicationethics.org/retraction-guidelines and a DOI to this https://doi.org/10.24318/cope.2019.1.4 Figure 51: Brainerd &amp; You (2018) Figure 52: Brainerd &amp; You (2018) Figure 53: Brainerd &amp; You (2018) Figure 54: Brainerd &amp; You (2018) PubPeer https://pubpeer.com/static/faq might make an interesting subject for a final project. Behaviors widely understood within science to be dishonest and unethical, but which fall outside the U.S. misconduct definition, seem to account for another 10%. Those behaviors include forged authorship, fake peer reviews, and failure to obtain approval from institutional review boards for research on human subjects or animals. (Such retractions have increased as a share of all retractions, and some experts argue the United States should expand its definition of scientific misconduct to cover those behaviors.) – Brainerd &amp; You (2018) Fanelli, D., Costas, R. &amp; Larivière, V. (2015). Misconduct Policies, Academic Culture and Career Stage, Not Gender or Pressures to Publish, Affect Scientific Integrity. PloS One, 10(6), e0127556. https://doi.org/10.1371/journal.pone.0127556 Discussion of Retraction Watch Journals can get hijacked! https://retractionwatch.com/the-retraction-watch-hijacked-journal-checker/ https://docs.google.com/spreadsheets/d/1ak985WGOgGbJRJbZFanoktAN_UFeExpE/edit#gid=5255084 Retracted papers can garner large numbers of citations https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/ Exploring the database http://retractiondatabase.org/RetractionSearch.aspx? Students could consider updating the story of a retracted paper as a final project. What happened to the author(s)? Discussion of Office of Research Integrity (ORI) https://ori.hhs.gov/ Grant at Penn State: Project Title: Education and Assessment to Improve Research Misconduct Proceedings Grantee: The Pennsylvania State University - University Park Principal Investigator: Courtney Karmelita, BS, M.Ed., D.Ed. Institution (PI): The Pennsylvania State University - University Park Co-Principal Investigator: Bridget Carruthers, Ph.D., RBP Institution (Co-PI): The Ohio State University Abstract: The focus area of this proposal is “Handling allegations of research misconduct under 42 C.F.R. Part 93”. the proposed research will address the need for education and resource development for individuals assisting with the handling of research misconduct allegations at the inquiry or investigation phases. To date, there has been no published research on this topic as most Responsible Conduct of Research (RCR) training emphasizes the prevention of research misconduct rather than the policy and processes that govern the handling of research misconduct allegations. The study team will conduct a needs assessment through benchmarking with other institutions to identify and prioritize the creation of educational materials and trainings. The study team will then use the needs assessment to create online modules to provide foundational knowledge of research misconduct processes, definitions, and procedures for inquiry officials and investigation committee members. The study team will then implement the newly created resources and seek feedback from the appropriate stakeholders. In addition, the study team will explore to what extent, if any, standing committees develop a deeper understanding of the research misconduct process. This will help to inform which, if either, committee type is more efficacious. Research on standing versus ad hoc committees is also lacking in the literature. This proposed research has the potential to positively impact the research integrity community at large with much needed tools, resources, training and research on the management of research misconduct allegations. https://ori.hhs.gov/blog/ori-awards-three-research-integrity-grants Next time… Questionable research practices Read Simmons, Nelson, &amp; Simonsohn (2011) John, Loewenstein, &amp; Prelec (2012) Watch Ngiam (2020) References Brainerd, J., &amp; You, J. (2018). What a massive database of retracted papers reveals about science publishing’s “death penalty.” Science. https://doi.org/10.1126/science.aav8384 John, L. K., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953 Ngiam, W. (2020, April). ReproducibiliTea | simmons, nelson and simonsohn (2011). False-Positive psychology. Youtube. Retrieved from https://www.youtube.com/watch?v=bf3GqyBRgzY Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 "],["questionable-research-practices.html", "Questionable research practices Roadmap Central questions Background Simmons et al. (2011) John et al. (2012) Reproducibility notes Next time…", " Questionable research practices Roadmap Read Simmons et al. (2011) John et al. (2012) Watch Ngiam (2020) Remember Exercise 02: P-hack your way to scientific glory Due next Tuesday, February 21, 2023. Central questions What are researcher ‘degrees of freedom’? Why should we care about them? What are questionable research practices? Why should we care about them? Background False positives vs. false negatives Null hypothesis: There is no effect. Positive: What’s actually true, a true fact Negative: What’s actually not-true or false, a false fact What does our evidence say? Figure 55: False positives vs. false negatives Evidence says True fact False fact True True positive False positive (Type I) False False negative (Type II) True negative Goals: Minimize false positives (Type I errors) Minimize false negatives (Type II errors) Practices Set small p-value to reject null hypothesis if using null hypothesis significance testing (NHST) Control alpha \\(\\alpha\\). Control beta \\(\\beta\\) or statistical ‘power’ \\(1-\\beta\\). Researcher ‘degrees of freedom’ The culprit is a construct we refer to as researcher degrees of freedom. In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both? It is rare, and sometimes impractical, for researchers to make all these decisions beforehand. Rather, it is common (and accepted practice) for researchers to explore various analytic alternatives, to search for a combination that yields “statistical significance,” and to then report only what “worked.” The problem, of course, is that the likelihood of at least one (of many) analyses producing a falsely positive finding at the 5% level is necessarily greater than 5%. This exploratory behavior is not the by-product of malicious intent, but rather the result of two factors: (a) ambiguity in how best to make these decisions and (b) the researcher’s desire to find a statistically significant result. – Simmons et al. (2011) Choosing dependent measure or measures Choosing sample size Stopping early Choosing covariates Reporting subsets of conditions Trimming data (e.g., how to treat outliers) In a perusal of roughly 30 Psychological Science articles, we discovered considerable inconsistency in, and hence considerable ambiguity about, this decision. Most (but not all) researchers excluded some responses for being too fast, but what constituted “too fast” varied enormously: the fastest 2.5%, or faster than 2 standard deviations from the mean, or faster than 100 or 150 or 200 or 300 ms. Similarly, what constituted “too slow” varied enormously: the slowest 2.5% or 10%, or 2 or 2.5 or 3 standard deviations slower than the mean, or 1.5 standard deviations slower from that condition’s mean, or slower than 1,000 or 1,200 or 1,500 or 2,000 or 3,000 or 5,000 ms. None of these decisions is necessarily incorrect, but that fact makes any of them justifiable and hence potential fodder for self-serving justifications. – Simmons et al. (2011) Simmons et al. (2011) The studies Simulations These simulations assessed the impact of four common degrees of freedom: flexibility in (a) choosing among dependent variables, (b) choosing sample size, (c) using covariates, and (d) reporting subsets of experimental conditions. We also investigated various combinations of these degrees of freedom. We generated random samples with each observation independently drawn from a normal distribution, performed sets of analyses on each sample, and observed how often at least one of the resulting p values in each sample was below standard significance levels. For example, imagine a researcher who collects two dependent variables, say liking and willingness to pay. The researcher can test whether the manipulation affected liking, whether the manipulation affected willingness to pay, and whether the manipulation affected a combination of these two variables. The likelihood that one of these tests produces a significant result is at least somewhat higher than .05. We conducted 15,000 simulations of this scenario (and other scenarios) to estimate the size of “somewhat. – Simmons et al. (2011) Findings Figure 56: Table 1 from Simmons et al. (2011) Researcher choices inflate false positive rate above 5% (\\(p&lt;.05\\)). Figure 57: Figure 1 from Simmons et al. (2011) Collecting more data after analyzing a small initial sample inflates the false positive rate. Figure 58: Figure 2 from Simmons et al. (2011) Just because a statistical test met the criterion threshold with a small sample doesn’t mean it will with larger samples. Recommendations For authors Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. Following this requirement may mean reporting the outcome of power calculations or disclosing arbitrary rules, such as “we decided to collect 100 observations” or “we decided to collect as many observations as we could before the end of the semester.” The rule itself is secondary, but it must be determined ex ante and be reported. Authors must collect at least 20 observations per cell or else provide a compelling cost-of-data-collection justification. This requirement offers extra protection for the first requirement. Samples smaller than 20 per cell are simply not powerful enough to detect most effects, and so there is usually no good reason to decide in advance to collect such a small number of observations. Smaller samples, it follows, are much more likely to reflect interim data analysis and a flexible termination rule. In addition, as Figure 1 shows, larger minimum sample sizes can lessen the impact of violating Requirement 1. Authors must list all variables collected in a study. This requirement prevents researchers from reporting only a convenient subset of the many measures that were collected, allowing readers and reviewers to easily identify possible researcher degrees of freedom. Because authors are required to just list those variables rather than describe them in detail, this requirement increases the length of an article by only a few words per otherwise shrouded variable. We encourage authors to begin the list with “only,” to assure readers that the list is exhaustive (e.g., “participants reported only their age and gender”). Authors must report all experimental conditions, including failed manipulations. This requirement prevents authors from selectively choosing only to report the condition comparisons that yield results that are consistent with their hypothesis. As with the previous requirement, we encourage authors to include the word “only” (e.g., “participants were randomly assigned to one of only three conditions”). If observations are eliminated, authors must also report what the statistical results are if those observations are included. This requirement makes transparent the extent to which a finding is reliant on the exclusion of observations, puts appropriate pressure on authors to justify the elimination of data, and encourages reviewers to explicitly consider whether such exclusions are warranted. Correctly interpreting a finding may require some data exclusions; this requirement is merely designed to draw attention to those results that hinge on ex post decisions about which data to exclude. If an analysis includes a covariate, authors must report the statistical results of the analysis without the covariate. Reporting covariate-free results makes transparent the extent to which a finding is reliant on the presence of a covariate, puts appropriate pressure on authors to justify the use of the covariate, and encourages reviewers to consider whether including it is warranted. Some findings may be persuasive even if covariates are required for their detection, but one should place greater scrutiny on results that do hinge on covariates despite random assignment. Simmons et al. (2011) For reviewers Reviewers should ensure that authors follow the requirements. Review teams are the gatekeepers of the scientific community, and they should encourage authors not only to rule out alternative explanations, but also to more convincingly demonstrate that their findings are not due to chance alone. This means prioritizing transparency over tidiness; if a wonderful study is partially marred by a peculiar exclusion or an inconsistent condition, those imperfections should be retained. If reviewers require authors to follow these requirements, they will. Reviewers should be more tolerant of imperfections in results. One reason researchers exploit researcher degrees of freedom is the unreasonable expectation we often impose as reviewers for every data pattern to be (significantly) as predicted. Underpowered studies with perfect results are the ones that should invite extra scrutiny. Reviewers should require authors to demonstrate that their results do not hinge on arbitrary analytic decisions. Even if authors follow all of our guidelines, they will necessarily still face arbitrary decisions. For example, should they subtract the baseline measure of the dependent variable from the final result or should they use the baseline measure as a covariate? When there is no obviously correct way to answer questions like this, the reviewer should ask for alternatives. For example, reviewer reports might include questions such as, “Do the results also hold if the baseline measure is instead used as a covariate?” Similarly, reviewers should ensure that arbitrary decisions are used consistently across studies (e.g., “Do the results hold for Study 3 if gender is entered as a covariate, as was done in Study 2?”).5 If a result holds only for one arbitrary specification, then everyone involved has learned a great deal about the robustness (or lack thereof) of the effect. If justifications of data collection or analysis are not compelling, reviewers should require the authors to conduct an exact replication. If a reviewer is not persuaded by the justifications for a given researcher degree of freedom or the results from a robustness check, the reviewer should ask the author to conduct an exact replication of the study and its analysis. We realize that this is a costly solution, and it should be used selectively; however, “never” is too selective. Simmons et al. (2011) Illustration Figure 59: Table 3 from Simmons et al. (2011) First, notice that in our original report, we redacted the many measures other than father’s age that we collected (including the dependent variable from Study 1: feelings of oldness). A reviewer would hence have been unable to assess the flexibility involved in selecting father’s age as a control. Second, by reporting only results that included the covariate, we made it impossible for readers to discover its critical role in achieving a significant result. Seeing the full list of variables now disclosed, reviewers would have an easy time asking for robustness checks, such as “Are the results from Study 1 replicated in Study 2?” They are not: People felt older rather than younger after listening to “When I’m Sixty-Four,” though not significantly so, F(1, 17) = 2.07, p = .168. Finally, notice that we did not determine the study’s termination rule in advance; instead, we monitored statistical significance approximately every 10 observations. Moreover, our sample size did not reach the 20-observation threshold set by our requirements. The redacted version of the study we reported in this article fully adheres to currently acceptable reporting standards and is, not coincidentally, deceptively persuasive. The requirement-compliant version reported in Table 3 would be—appropriately—all but impossible to publish. – Simmons et al. (2011) John et al. (2012) Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm. – John et al. (2012) Paper not openly accessible. Paper was accessible via PSU library. Results Figure 60: Table 1 from John et al. (2012) The two versions of the survey differed in the incentives they offered to respondents. In the Bayesian-truth-serum (BTS) condition, a scoring algorithm developed by one of the authors (Prelec, 2004) was used to provide incentives for truth telling. This algorithm uses respondents’ answers about their own behavior and their estimates of the sample distribution of answers as inputs in a truth-rewarding scoring formula. Because the survey was anonymous, compensation could not be directly linked to individual scores. Instead, respondents were told that we would make a donation to a charity of their choice, selected from five options, and that the size of this donation would depend on the truthfulness of their responses, as determined by the BTS scoring system. By inducing a (correct) belief that dishonesty would reduce donations, we hoped to amplify the moral stakes riding on each answer (for details on the donations, see Supplementary Results in the Supplemental Material). Respondents were not given the details of the scoring system but were told that it was based on an algorithm published in Science and were given a link to the article. There was no deception: Respondents’ BTS scores determined our contributions to the five charities. Respondents in the control condition were simply told that a charitable donation would be made on behalf of each respondent. (For details on the effect of the size of the incentive on response rates, see Participation Incentive Survey in the Supplemental Material.) – John et al. (2012) Figure 61: Figure 1 from John et al. (2012) Figure 62: Figure 2 from John et al. (2012) Reproducibility notes Simmons et al. (2011) Simmons et al. (2011) does not appear to have shared data or any supplementary materials along with the article. John et al. (2012) Supplemental Material Additional supporting information may be found at http://pss.sagepub.com/content/by/supplemental-data – John et al. (2012) Visiting this URL (http://pss.sagepub.com/content/by/supplemental-data) takes one to the following: I went to the journal page and searched for the article title. Since the article is behind a paywall, I wasn’t able to access the supplemental materials that way. After authenticating to the PSU library, I was able to find a PDF of the supplementary material. It and the original paper are on Canvas as of 2023-02-15. I was unable to find the specific survey questions or the data. Next time… Discuss: Exercise 02: P-hack your way to scientific glory, due next time. Bias (Ritchie, 2020), Chapter 4 References John, L. K., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953 Ngiam, W. (2020, April). ReproducibiliTea | simmons, nelson and simonsohn (2011). False-Positive psychology. Youtube. Retrieved from https://www.youtube.com/watch?v=bf3GqyBRgzY Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 "],["bias.html", "Bias Roadmap Kinds of bias Discuss p-hacking exercise Next time…", " Bias Figure 63: T. Baker (2023) Roadmap Discuss (Ritchie, 2020), Chapter 4 Discuss results from Exercise 02: P-hack your way to scientific glory. Due today Exercise 02: P-hack your way to scientific glory Kinds of bias Analysts’ prior assumptions Researcher, reviewer, and publication bias toward positive (vs. negative/null) results File-drawer effect Preference for simple results (c.f., Stapel) Overfitting Conflicts of interest p-hacking Discuss p-hacking exercise Who got a “significant” result? How many different analyses did you try? Who changed their analysis after finding a significant result? Did anyone try another analysis–after you got a significant result–and keep the non-significant result? Preparation It often saves typing to load a set of commands into memory. In R, groups of useful commands are called ‘packages’. We can load a set of useful packages into memory by issuing the following command. If you are interested in a career related to data science, tidyverse is a very powerful set of tools you will want to know more about. Data entry Via a Google Sheet: https://docs.google.com/spreadsheets/d/1fnSwFrUcKvgqq_agDLe4t2DHXHtHoOlmLdtLVRSemrI/edit?usp=sharing Gilmore added data validation (Format/Data Validation) to the columns. Why? These data are “long”. Each row is a unique observation. Long data are often easier to work with. But not always. Data gathering First, I authenticate (sign-in) to Google using my Gmail account. If I haven’t logged in using this script recently, it will ask me to log-in again. Then I download the Google Sheet to a directory/folder called csv/ using the file name p-hacking.csv. ## File downloaded: ## • &#39;PSYCH 490.002 2023 P-hacking&#39; &lt;id: 1fnSwFrUcKvgqq_agDLe4t2DHXHtHoOlmLdtLVRSemrI&gt; ## Saved locally as: ## • &#39;]8;;file:///Users/rick/rrr/psych-490-reproducibility-2023-spring-notes/src/csv/p-hacking.csvcsv/p-hacking.csv]8;;&#39; What does CSV mean? Why are CSV files often used in data analysis? Next, I read the CSV file using the read_csv() function. Functions in R take inputs and deliver outputs. The inputs are file and show_col_types. The output is an object called p_hacking. It is a table of data that I can refer to with that name. I like to use the ‘structure’ function or str() to see what the data look like. Data is a plural noun. So, (when we don’t forget this) we say ‘The data are…’ not ‘The data is…’. ## spc_tbl_ [23 × 16] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ student : num [1:23] 0 0 0 1 1 1 2 3 4 4 ... ## $ analysis : num [1:23] 1 2 3 1 2 3 1 1 1 2 ... ## $ party : chr [1:23] &quot;democrats&quot; &quot;democrats&quot; &quot;democrats&quot; &quot;democrats&quot; ... ## $ prediction : chr [1:23] &quot;worse&quot; &quot;worse&quot; &quot;worse&quot; &quot;better&quot; ... ## $ power_president : logi [1:23] FALSE FALSE FALSE TRUE TRUE TRUE ... ## $ power_governors : logi [1:23] FALSE FALSE FALSE TRUE FALSE TRUE ... ## $ power_senators : logi [1:23] TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ power_reps : logi [1:23] TRUE TRUE TRUE FALSE FALSE FALSE ... ## $ econ_employment : logi [1:23] TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ econ_inflation : logi [1:23] FALSE FALSE FALSE FALSE FALSE TRUE ... ## $ econ_gdp : logi [1:23] FALSE FALSE FALSE TRUE TRUE TRUE ... ## $ econ_stocks : logi [1:23] TRUE TRUE TRUE FALSE FALSE FALSE ... ## $ factor_in_power : logi [1:23] FALSE TRUE FALSE TRUE TRUE TRUE ... ## $ exclude_recessions: logi [1:23] FALSE FALSE TRUE TRUE TRUE TRUE ... ## $ p_value : num [1:23] 0.06 0.19 0.02 0.02 0.21 0.32 0.01 0.01 0.18 0.1 ... ## $ publishable : chr [1:23] &quot;no&quot; &quot;no&quot; &quot;yes&quot; &quot;yes&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. student = col_double(), ## .. analysis = col_double(), ## .. party = col_character(), ## .. prediction = col_character(), ## .. power_president = col_logical(), ## .. power_governors = col_logical(), ## .. power_senators = col_logical(), ## .. power_reps = col_logical(), ## .. econ_employment = col_logical(), ## .. econ_inflation = col_logical(), ## .. econ_gdp = col_logical(), ## .. econ_stocks = col_logical(), ## .. factor_in_power = col_logical(), ## .. exclude_recessions = col_logical(), ## .. p_value = col_double(), ## .. publishable = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Questions to explore Most data analysts find that the process of exploring data is iterative. We start with a question. That leads to another question. That leads to yet another question. It is also sometimes cyclical. To answer a question requires that we modify the form of our data file. I like to start with thinking about “data pictures.” If X was true, what would the data look like? The following code we may or may not use. I put it here so it’s easier for all of us if we need to make use of it. Visualize How many different combinations of variable choices are there? There are \\(n=4\\) measures of political control; \\(n=4\\) measures of economic performance; \\(n=2\\) ‘other’ factors; \\(n=2\\) prediction choices; and \\(n=2\\) political parties to focus on. We can use the combinat package to help us figure this out. ## [,1] [,2] [,3] [,4] ## [1,] &quot;pres&quot; &quot;gov&quot; &quot;senate&quot; &quot;house&quot; This shows us the number of ways we can pick a single political measure from among the 4 choices. We see that there are 4 ways. The next function shows us the number of ways to pick two measures. ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] &quot;pres&quot; &quot;pres&quot; &quot;pres&quot; &quot;gov&quot; &quot;gov&quot; &quot;senate&quot; ## [2,] &quot;gov&quot; &quot;senate&quot; &quot;house&quot; &quot;senate&quot; &quot;house&quot; &quot;house&quot; There are 6 columns of two, so there must be 6 different ways to pick two measures. ## [,1] [,2] [,3] [,4] ## [1,] &quot;pres&quot; &quot;pres&quot; &quot;pres&quot; &quot;gov&quot; ## [2,] &quot;gov&quot; &quot;gov&quot; &quot;senate&quot; &quot;senate&quot; ## [3,] &quot;senate&quot; &quot;house&quot; &quot;house&quot; &quot;house&quot; There are 4 different ways to pick 3 measures. And there is only one way to pick 4 among 4. Make sense? If we add these up ‘4 + 6 + 4 + 1’ = 15 we get the number of different choices we can make (15) about how many combinations of political power measures are possible. Since there are also 4 different choices of economic performance measures, we know that there are 15 ways to pick these. Now we can calculate how many different possible combinations of variables there are. We multiply because each of the choices (political power, economic performance, party, better or worse is independent). So, there are \\(n=\\) 1800 of variables we could have chosen. No wonder different students got such different results! Next time… File drawer effect Rosenthal (1979) Franco, Malhotra, &amp; Simonovits (2014) Work session on final project proposals, due Thursday, March 2. References Baker, T. (2023, February 17). Review found “falsified data” in stanford president’s research, colleagues allege. https://stanforddaily.com/2023/02/17/internal-review-found-falsified-data-in-stanford-presidents-alzheimers-research-colleagues-allege/. Retrieved from https://stanforddaily.com/2023/02/17/internal-review-found-falsified-data-in-stanford-presidents-alzheimers-research-colleagues-allege/ Franco, A., Malhotra, N., &amp; Simonovits, G. (2014). Social science. Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638 "],["file-drawer-effect.html", "File drawer effect Roadmap On p-hacking File drawer effect Discuss Rosenthal (1979) Discuss Franco et al. (2014) Next time…", " File drawer effect Figure 64: Economist (2023) Roadmap Take-home messages from p-hacking exercise Discussion about the file drawer effect Rosenthal (1979) Franco et al. (2014) Work session on final project proposals, due Thursday, March 2. On p-hacking What did we learn? Does the economy do better or worse under Republican or Democratic control? Does the answer depend on what measures (of political control, of economic performance) used? When does it matter if someone reporting on an issue engaged in p-hacking? File drawer effect Figure 65: https://www.craigmarker.com/wp-content/uploads/2006/12/filedrawer1-1002x675.jpg Related to the notion of ‘negative data’: Figure 66: Hypothesis (2021) Simulating the file drawer effect How many times out of \\(n\\) experiments do we get… a significant result a non-significant result Simulation app: https://rogilmore.shinyapps.io/PSYCH490-2023-APES/ Pick small effect size \\(d=0.30\\) There is a true effect, \\(B &gt; A\\) Pick samples of \\(n=30\\) Discuss Rosenthal (1979) Abstract For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the “file drawer problem” is that journals are filled with the 5% of the studies that show Type I errors, while the file drawers are filled with the 95% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. – Rosenthal (1979) The extreme view of this problem, the “file drawer problem,” is that the journals are filled with the 5% of the studies that show Type I errors, while the file drawers back at the lab are filled with the 95% of the studies that show nonsignificant (e.g., p &gt; .05) results. – Rosenthal (1979) Rosenthal’s illustration relies on the standard normal deviate or \\(Z\\), which is the value on a standard normal distribution (with mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\)) that one would need to observe for a given p value. n &lt;- 1000 p_val &lt;- 0.05 df &lt;- tibble::tibble(x = rnorm(n, mean = 0, sd = 1)) qt_05 &lt;- qt(p_val, n, lower.tail = FALSE) ggplot(df) + aes(x) + geom_histogram(bins = 20) + geom_vline(xintercept = qt_05) + ggtitle(paste0(&quot;Z=&quot;, format(qt_05, digits = 3, nsmall = 2), &quot; for p=&quot;, format(p_val, digits = 3, nsmall = 2), &quot; and n=&quot;, n)) Figure 67: Illustration of \\(Z\\) discussed in Rosenthal (1979) Findings If the overall level of significance of the research review will be brought down to the level of just significant by the addition of just a few more null results, the finding is not resistant to the file drawer threat. – Rosenthal (1979) There is both a sobering and a cheering lesson to be learned from careful study of Equation 3. The sobering lesson is that small numbers of studies that are not very significant, even when their combined p is significant, may well be misleading in that only a few studies filed away could change the combined significant result to a nonsignificant one…The cheering lesson is that when the number of studies available grows large or the mean directional Z grows large, the file drawer hypothesis as a plausible rival hypothesis can be safely ruled out. – Rosenthal (1979) Discuss Franco et al. (2014) Franco, A., Malhotra, N. &amp; Simonovits, G. (2014). Social science. Publication bias in the social sciences: unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484 Abstract We studied publication bias in the social sciences by analyzing a known population of conducted studies—221 in total—in which there is a full accounting of what is published and unpublished. We leveraged Time-sharing Experiments in the Social Sciences (TESS), a National Science Foundation–sponsored program in which researchers propose survey-based experiments to be run on representative samples of American adults. Because TESS proposals undergo rigorous peer review, the studies in the sample all exceed a substantial quality threshold. Strong results are 40 percentage points more likely to be published than are null results and 60 percentage points more likely to be written up. We provide direct evidence of publication bias and identify the stage of research production at which publication bias occurs: Authors do not write up and submit null findings. – Franco et al. (2014) Findings Figure 68: Table 3 from Franco et al. (2014) Solutions How can the social science community combat publication bias of this sort? On the basis of communications with the authors of many experiments that resulted in null findings, we found that some researchers anticipate the rejection of such papers but also that many of them simply lose interest in “unsuccessful” projects. These findings show that a vital part of developing institutional solutions to improve scientific transparency would be to understand better the motivations of researchers who choose to pursue projects as a function of results. Few null findings ever make it to the review process. Hence, proposed solutions such as two-stage review (the first stage for the design and the second for the results), pre-analysis plans (41), and requirements to preregister studies (16) should be complemented by incentives not to bury statistically insignificant results in file drawers. Creating high-status publication outlets for these studies could provide such incentives. The movement toward open-access journals may provide space for such articles. Further, the pre-analysis plans and registries themselves will increase researcher access to null results. Alternatively, funding agencies could impose costs on investigators who do not write up the results of funded studies. Last, resources should be deployed for replications of published studies if they are unrepresentative of conducted studies and more likely to report large effects. – Franco et al. (2014) Replication notes Paper was behind the standard Science paywall. Data &amp; code shared on Zenodo https://doi.org/10.5281/zenodo.11300 Let’s try to run it… ## [1] &quot;id&quot; &quot;author1field&quot; &quot;author2field&quot; ## [4] &quot;author3field&quot; &quot;discipline&quot; &quot;discipline2&quot; ## [7] &quot;POL&quot; &quot;PSY&quot; &quot;SOC&quot; ## [10] &quot;OTHER&quot; &quot;year&quot; &quot;fieldingperiod1stday&quot; ## [13] &quot;age_months&quot; &quot;age_day&quot; &quot;age_year&quot; ## [16] &quot;timetopublish&quot; &quot;iv_coder1&quot; &quot;iv_coder2&quot; ## [19] &quot;disagreement&quot; &quot;IV_all&quot; &quot;IV&quot; ## [22] &quot;strong&quot; &quot;mixed&quot; &quot;null&quot; ## [25] &quot;DV&quot; &quot;DV_all&quot; &quot;DV_tri&quot; ## [28] &quot;DV_book_separate&quot; &quot;DV_book_unpub&quot; &quot;DV_book_nontop&quot; ## [31] &quot;DV_book_top&quot; &quot;journal&quot; &quot;journal_field&quot; ## [34] &quot;insample&quot; &quot;pubyear&quot; &quot;pub&quot; ## [37] &quot;anyresults&quot; &quot;written&quot; &quot;max_h_current&quot; ## [40] &quot;max_pub_attime&quot; &quot;why_excluded&quot; ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: bounds ## X-squared = 70.503, df = 1, p-value &lt; 2.2e-16 ## quartz_off_screen ## 2 Figure 69: Figure generated from code in https://doi.org/10.5281/zenodo.11300 So, we can regenerate one of the figures (S1, p. 6) in the Supplemental Material. More could be done with these data. This could be a final project for someone. Next time… Negligence (Ritchie, 2020), Chapter 5 Nuijten, Hartgerink, Assen, Epskamp, &amp; Wicherts (2015) Szucs &amp; Ioannidis (2017) Work session on final project proposals, due Thursday, March 2. References Economist. (2023). There is a worrying amount of fraud in medical research. The Economist. Retrieved from https://www.economist.com/science-and-technology/2023/02/22/there-is-a-worrying-amount-of-fraud-in-medical-research Franco, A., Malhotra, N., &amp; Simonovits, G. (2014). Social science. Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484 Hypothesis, B. A. S. (2021, February). 13. “Negative data” and the file drawer problem. Youtube. Retrieved from https://www.youtube.com/watch?v=9I1qR8PTr54 Nuijten, M. B., Hartgerink, C. H. J., Assen, M. A. L. M. van, Epskamp, S., &amp; Wicherts, J. M. (2015). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 1–22. https://doi.org/10.3758/s13428-015-0664-2 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638 Szucs, D., &amp; Ioannidis, J. P. A. (2017). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. PLoS Biology, 15(3), e2000797. https://doi.org/10.1371/journal.pbio.2000797 "],["negligence.html", "Negligence In the news Roadmap Types of negligence Discussion of Exercise 03: Alpha, Power, Effect Sizes, &amp; Sample Size Next time…", " Negligence In the news Youyou, Yang, &amp; Uzzi (2023) Figure 70: Youyou et al. (2023) Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (N = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper’s likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors’ cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author’s university prestige and a paper’s citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability. – Youyou et al. (2023) Roadmap Discuss (Ritchie, 2020), Chapter 5 Nuijten et al. (2015) Szucs &amp; Ioannidis (2017) Discuss Assignment Exercise 03: Alpha, Power, Effect Sizes, &amp; Sample Size, due Thursday, March 16 Work session proposals, due Thursday, March 2. Types of negligence Definitions of negligent Figure 71: negligence from Mac OS dictionary app Data mistakes e.g., Reinhart &amp; Rogoff spreadsheet Statistical reporting errors e.g., Nuijten et al. (2015)) “This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package “statcheck.” statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called “co-pilot model,” and to use statcheck to flag possible inconsistencies in one’s own manuscript or during the review process.” – Nuijten et al. (2015) statcheck https://michelenuijten.shinyapps.io/statcheck-web/ Granularity-Releated Inconsistency of Means (GRIM) Brown &amp; Heathers (2017) “We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.” – Brown &amp; Heathers (2017) A possible final project might involve assessing some retracted papers using either statcheck or GRIMM. It would be interesting to see whether the tools could have detected problems in advance of publication. Non-random sampling, blinding and related issues e.g., Carlisle (2017) but see Kharasch &amp; Houle (2018) and reply Carlisle (2018) Inadequate power Power: If there is an effect, what’s the probability my test/decision procedure will detect it (avoid a false negative). If \\(\\beta\\) is \\(p\\)(false negative), then power is \\(1-\\beta\\). Sample size and alpha (\\(\\alpha\\)) or \\(p\\)(false positive) affect power, as does the actual (unknown in advance) effect size (\\(d\\)). Conventions for categorizing effect sizes: small (\\(d\\) = 0.2), medium (\\(d\\) = 0.5), and large (\\(d\\) = 0.8). “We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64–1.46) for nominally statistically significant results and D = 0.24 (0.11–0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.” – Szucs &amp; Ioannidis (2017) Figure 72: Figure 3 from Szucs &amp; Ioannidis (2017) Discussion of Exercise 03: Alpha, Power, Effect Sizes, &amp; Sample Size Goal To gain a better understanding of how these concepts relate to one another and affect statistical decision-making. App https://rogilmore.shinyapps.io/PSYCH490-2023-APES/ Next time… Hype (Ritchie, 2020), Chapter 6 Carney, Cuddy, &amp; Yap (2010) (Optional) Ranehill et al. (2015) Watch Cuddy (2012) Due Final project proposal References Brown, N. J. L., &amp; Heathers, J. A. J. (2017). The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology. Social Psychological and Personality Science, 8(4), 363–369. https://doi.org/10.1177/1948550616673876 Carlisle, J. B. (2017). Data fabrication and other reasons for non-random sampling in 5087 randomised, controlled trials in anaesthetic and general medical journals. Anaesthesia, 72(8), 944–952. https://doi.org/10.1111/anae.13938 Carlisle, J. B. (2018). Seeking and reporting apparent research misconduct: Errors and integrity - a reply. Anaesthesia, 73(1), 126–128. https://doi.org/10.1111/anae.14148 Carney, D. R., Cuddy, A. J. C., &amp; Yap, A. J. (2010). Power posing: Brief nonverbal displays affect neuroendocrine levels and risk tolerance. Psychological Science, 21(10), 1363–1368. https://doi.org/10.1177/0956797610383437 Cuddy, A. (2012). Your body language may shape who you are. Retrieved from https://www.ted.com/talks/amy_cuddy_your_body_language_may_shape_who_you_are Kharasch, E. D., &amp; Houle, T. T. (2018). Seeking and reporting apparent research misconduct: Errors and integrity. Anaesthesia, 73(1), 125–126. https://doi.org/10.1111/anae.14147 Nuijten, M. B., Hartgerink, C. H. J., Assen, M. A. L. M. van, Epskamp, S., &amp; Wicherts, J. M. (2015). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 1–22. https://doi.org/10.3758/s13428-015-0664-2 Ranehill, E., Dreber, A., Johannesson, M., Leiberg, S., Sul, S., &amp; Weber, R. A. (2015). Assessing the robustness of power posing: No effect on hormones and risk tolerance in a large sample of men and women. Psychological Science, 26(5), 653–656. https://doi.org/10.1177/0956797614553946 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Szucs, D., &amp; Ioannidis, J. P. A. (2017). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. PLoS Biology, 15(3), e2000797. https://doi.org/10.1371/journal.pbio.2000797 Youyou, W., Yang, Y., &amp; Uzzi, B. (2023). A discipline-wide investigation of the replicability of psychology papers over the past two decades. Proceedings of the National Academy of Sciences of the United States of America, 120(6), e2208863120. https://doi.org/10.1073/pnas.2208863120 "],["hype.html", "Hype In the news Roadmap (Ritchie, 2020), Chapter 6 Carney et al. (2010) Cuddy (2012) Ranehill et al. (2015) Exploring logical/causal diagrams Next time…", " Hype In the news Youyou, W., Yang, Y. &amp; Uzzi, B. (2023). A discipline-wide investigation of the replicability of Psychology papers over the past two decades. Proceedings of the National Academy of Sciences of the United States of America, 120(6), e2208863120. https://doi.org/10.1073/pnas.2208863120 Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (N = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper’s likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors’ cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author’s university prestige and a paper’s citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability. – Youyou et al. (2023) Roadmap Discuss (Ritchie, 2020), Chapter 6 Carney et al. (2010) (Optional) Ranehill et al. (2015) Watch/discuss Cuddy (2012) Due Final project proposals. Reminder Assignment Exercise 03: Alpha, Power, Effect Sizes, &amp; Sample Size, due Tuesday, March 16 The New York Times published an article (Dominus (2017)) on the controversy surrounding Dr. Cuddy’s work called “When the Revolution Came for Amy Cuddy.” I considered reading this and discussing it, but I was more interested in the substantive claims made in the papers and in the talk. Reading the Times’ story would make a great final project topic, however. (Ritchie, 2020), Chapter 6 Problems with scientific press releases Not written by scientists give unwarranted advice cross-species leaps or generalizations equating correlation with causation “Churnalism” Journalists do not do their own investigations but repeat press releases Popular books (by scientists, too) can gloss over complexities, nuances Positive rhetoric/spin Counter-example from OPERA study of faster-than-light particle Carney et al. (2010) Carney, D. R., Cuddy, A. J. C. &amp; Yap, A. J. (2010). Power posing: brief nonverbal displays affect neuroendocrine levels and risk tolerance. Psychological Science, 21(10), 1363–1368. https://doi.org/10.1177/0956797610383437 Humans and other animals express power through open, expansive postures, and they express powerlessness through contractive postures. But can these postures actually cause power? The results of this study confirmed our prediction posing in high-power nonverbal displays (as opposed to low-power nonverbal displays) would cause neuroendocrine behavioral changes for both male and female participants: High-power posers experienced elevations in testosterone, decreases in cortisol, and increased feelings of power and tolerance for risk; low-power posers exhibited the opposite pattern. In posing in displays of power caused advantaged and adaptive psychological, physiological, and behavioral changes, and findings suggest that embodiment extends beyond mere thinking and feeling, to physiology and subsequent behavioral That a person can, by assuming two simple 1-min poses, embody power and instantly become more powerful has real-actionable implications. – Carney et al. (2010) Methods \\(n=42\\) participants (26 female; age or other demographic characteristics?) Assigned into two groups (high power vs. low power) Experimenter posed bodies Two different poses held for 1 min Figure 73: High power poses. Figure 1 from Carney et al. (2010) Figure 74: Low power poses. Figure 2 from Carney et al. (2010) Other tasks/measures Gambling task (risk-taking, powerful feelings) Self-reported feelings of power Saliva samples (pretest and ~ 17 min after pose) Tested for cortisol, testosterone Results Figure 75: Carney et al. (2010) Figure 76: Figure 3 from Carney et al. (2010) 0.0.1 Citation data Paper cited ~1,500 times (https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C39&amp;q=Power+posing%3A+brief+nonverbal+displays+affect+neuroendocrine+levels+and+risk+tolerance&amp;btnG=) according to Google Scholar on 2023-03-01. Reproducibility notes Article was behind a paywall. PDF was available via authenticated access to PSU Libraries. No data are shared; no code used to make the figures or conduct the analyses were shared. Presentation comments Better to show individual participant data points (see below) Cuddy (2012) Figure 77: Amy Cuddy’s TED talk: https://www.ted.com/talks/amy_cuddy_your_body_language_may_shape_who_you_are 0.0.2 Citation data Talk viewed ~67 million times according to TED web page on 2023-03-01. Ranehill et al. (2015) Ranehill, E., Dreber, A., Johannesson, M., Leiberg, S., Sul, S. &amp; Weber, R. A. (2015). Assessing the robustness of power posing: no effect on hormones and risk tolerance in a large sample of men and women [Review of Assessing the robustness of power posing: no effect on hormones and risk tolerance in a large sample of men and women]. Psychological Science, 26(5), 653–656. journals.sagepub.com. https://doi.org/10.1177/0956797614553946 We conducted a conceptual replication study task in both the gain dom with a similar methodology as that employed by Carney loss domain. In t et al. but using a substantially larger sample (N= 200) and a design in which the experimenter was blind to condition. Our statistical power to detect an effect of the magnitude reported by Carney et al. was more than 95% (see the Supplemental Material available online). In addition to the three outcome measures that Carney et al. used, we also studied two more behavioral tasks (risk taking in the loss domain and willingness to compete). Consistent with the findings of Carney et al., our results showed a significant effect of power posing on self-reported feelings of power. However, we found no significant effect of power posing on hormonal levels in any of the three behavioral tasks. – Ranehill et al. (2015) Methods Sampled \\(n=100\\) after power analysis; then sampled another 100. 3 min pose durations Participants modeled poses following computer instructions. No deception. Results Figure 78: Results excerpt from Ranehill et al. (2015) Figure 79: Figure 1 from Ranehill et al. (2015) 0.0.3 Citation data Paper cited ~314 times (https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C39&amp;q=Assessing+the+robustness+of+power+posing%3A+no+effect+on+hormones+and+risk+tolerance+in+a+large+sample+of+men+and+women+%5BReview+of+Assessing+the+robustness+of+power+posing%3A+no+effect+on+hormones+and+risk+tolerance+in+a+large+sample+of+men+and+women%5D&amp;btnG=) according to Google Scholar on 2023-03-01. Reproducibility notes Article was behind a paywall. The PDF was available via authenticated access to PSU Libraries. The PDF did not easily permit cutting and pasting of text, so it was hard to excerpt those for this document. The HTML version of the article was unavailable, so I had to make screenshots of figures. There is an OSF site with data and materials. The OSF site includes very helpful details about the experiments. The data and code were shared, but I don’t have access to the statistical program used (Stata) to rerun the analysis. The data had a .dta file extension. This appears to be a plain text, ‘tidy’ data file format that could be used by another program. Presentation comments It’s better to show individual participant data points in a ‘spaghetti’ plot. That way the read can see both the group effect and how individual participants fared. Here’s an example with made-up data. Figure 80: Example of ‘spaghetti’ plot. Exploring logical/causal diagrams Causal diagrams are one tool to reveal an authors’ claims and assumptions. Figure 81: Implicit causal model tested More typically, we think that feelings cause behaviors. Figure 82: Conventional causal model Or that feelings and physiological responses (e.g., hormones like cortisol and testosterone) together influence behaviors. Figure 83: Conventional causal model II Neuroscientists might argue that feelings are the outcome of physiological processes like hormone levels and nervous system activity. Also, physiological processes influence one another. Figure 84: Neuroscience-oriented causal model To understand the full system of influences, we might need to look at a more complicated causal model. Here, behaviors influence and are influenced by physiology. Behaviors influence, and are influenced by feelings through influences on physiology. However, the evidence presented in Carney et al. (2010) and Ranehill et al. (2015) show that ‘posing’ behavior influences feelings, but not (hormonal) physiology. If the neuroscientific view is correct, there must be another route for behaviors to influence feeling through physiology. That route is probably nervous system activity. Figure 85: Plausible fully interactive causal model Next time… Solutions Read Munafò et al. (2017) Begley (2013) References Begley, C. G. (2013). Six red flags for suspect work. Nature, 497(7450), 433–434. https://doi.org/10.1038/497433a Carney, D. R., Cuddy, A. J. C., &amp; Yap, A. J. (2010). Power posing: Brief nonverbal displays affect neuroendocrine levels and risk tolerance. Psychological Science, 21(10), 1363–1368. https://doi.org/10.1177/0956797610383437 Cuddy, A. (2012). Your body language may shape who you are. Retrieved from https://www.ted.com/talks/amy_cuddy_your_body_language_may_shape_who_you_are Dominus, S. (2017). When the revolution came for amy cuddy. The New York Times. Retrieved from https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Sert, N. P. du, … Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. https://doi.org/10.1038/s41562-016-0021 Ranehill, E., Dreber, A., Johannesson, M., Leiberg, S., Sul, S., &amp; Weber, R. A. (2015). Assessing the robustness of power posing: No effect on hormones and risk tolerance in a large sample of men and women. Psychological Science, 26(5), 653–656. https://doi.org/10.1177/0956797614553946 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 Youyou, W., Yang, Y., &amp; Uzzi, B. (2023). A discipline-wide investigation of the replicability of psychology papers over the past two decades. Proceedings of the National Academy of Sciences of the United States of America, 120(6), e2208863120. https://doi.org/10.1073/pnas.2208863120 "],["survey_01_results.html", "Survey 1 results Purpose Background Set-up Download data Clean data Visualize data", " Survey 1 results Purpose This document summarizes the analysis of data for Survey 1 on scientific norms and counter-norms (https://forms.gle/1zqzfNNXWyCgiDSJ9). This survey is part of the class discussion of adherence to norms and counternorms. Background The survey questions were derived from the Appendix in (Kardash &amp; Edwards, 2012). We thank these authors for publishing the survey questions in their article so that we could reuse them for our class. Set-up We load the required R packages. suppressPackageStartupMessages(library(&quot;tidyverse&quot;)) # for pipe %&gt;% suppressPackageStartupMessages(library(&quot;googledrive&quot;)) suppressPackageStartupMessages(library(&quot;magrittr&quot;)) suppressPackageStartupMessages(library(&quot;dplyr&quot;)) suppressPackageStartupMessages(library(&quot;ggplot2&quot;)) Download data Download the data file from the spreadsheet generated by the Google Form into a comma-separated value (CSV) file in a local directory called csv/. Authenticate to Google. googledrive::drive_auth(email = &quot;rick.o.gilmore@gmail.com&quot;) if (!dir.exists(&#39;csv&#39;)) { dir.create(&#39;csv&#39;) } csv_fn &lt;- &quot;csv/psych-490-2023-spring-survey-01.csv&quot; googledrive::drive_download(file = &#39;PSYCH 490.002 Survey 1 (Responses)&#39;, path = csv_fn, type = &#39;csv&#39;, overwrite = TRUE) Clean data Load the data file. if (file.exists(csv_fn)) { survey_01 &lt;- readr::read_csv(csv_fn, show_col_types = FALSE) } else { message(&quot;File not found: &quot;, csv_fn) survey_01 &lt;- NULL } Examine the data set as a whole. str(survey_01) ## spc_tbl_ [21 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Timestamp : chr [1:21] &quot;10/21/2022 16:07:25&quot; &quot;1/10/2023 11:48:53&quot; &quot;1/12/2023 11:43:32&quot; &quot;1/12/2023 11:43:54&quot; ... ## $ Scientists are generally motivated by the desire for knowledge and discovery, and not by the possibility of personal gain. : num [1:21] 3 4 3 3 3 4 3 3 5 3 ... ## $ Scientists make an attempt to consider all new evidence, hypotheses, theories, and innovations, even those that challenge or contradict their own work.: num [1:21] 2 2 3 4 3 4 2 2 5 3 ... ## $ Scientists generally assess new knowledge and its applications based on the reputation and past productivity of the individual or research group. : num [1:21] 4 3 4 3 4 4 3 4 4 4 ... ## $ Scientists openly share new findings with all colleagues. : num [1:21] 1 3 2 2 3 3 2 2 4 4 ... ## $ Scientists generally invest their careers in promoting their own most important findings, theories, or innovations. : num [1:21] 5 5 5 5 4 5 3 3 4 4 ... ## $ Scientists compete with others in the same field for funding and recognition of their achievements. : num [1:21] 2 5 4 2 5 4 4 4 3 5 ... ## $ Scientists generally evaluate research only on its merit (i.e., according to accepted standards of the field). : num [1:21] 4 4 3 4 3 2 3 4 2 3 ... ## $ Scientists emphasize the protection of their newest findings to ensure priority in publishing, patenting, or applications. : num [1:21] 2 4 4 1 5 3 4 4 4 5 ... ## $ If you wish to comment about the questions in this survey, you may do so here. You are not required to comment. : chr [1:21] &quot;Test&quot; NA NA NA ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Timestamp = col_character(), ## .. `Scientists are generally motivated by the desire for knowledge and discovery, and not by the possibility of personal gain.` = col_double(), ## .. `Scientists make an attempt to consider all new evidence, hypotheses, theories, and innovations, even those that challenge or contradict their own work.` = col_double(), ## .. `Scientists generally assess new knowledge and its applications based on the reputation and past productivity of the individual or research group.` = col_double(), ## .. `Scientists openly share new findings with all colleagues.` = col_double(), ## .. `Scientists generally invest their careers in promoting their own most important findings, theories, or innovations.` = col_double(), ## .. `Scientists compete with others in the same field for funding and recognition of their achievements.` = col_double(), ## .. `Scientists generally evaluate research only on its merit (i.e., according to accepted standards of the field).` = col_double(), ## .. `Scientists emphasize the protection of their newest findings to ensure priority in publishing, patenting, or applications.` = col_double(), ## .. `If you wish to comment about the questions in this survey, you may do so here. You are not required to comment.` = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Examine the variable names. if (is.null(survey_01)) { warning(&quot;Error loading data file&quot;) } else { names(survey_01) } ## [1] &quot;Timestamp&quot; ## [2] &quot;Scientists are generally motivated by the desire for knowledge and discovery, and not by the possibility of personal gain.&quot; ## [3] &quot;Scientists make an attempt to consider all new evidence, hypotheses, theories, and innovations, even those that challenge or contradict their own work.&quot; ## [4] &quot;Scientists generally assess new knowledge and its applications based on the reputation and past productivity of the individual or research group.&quot; ## [5] &quot;Scientists openly share new findings with all colleagues.&quot; ## [6] &quot;Scientists generally invest their careers in promoting their own most important findings, theories, or innovations.&quot; ## [7] &quot;Scientists compete with others in the same field for funding and recognition of their achievements.&quot; ## [8] &quot;Scientists generally evaluate research only on its merit (i.e., according to accepted standards of the field).&quot; ## [9] &quot;Scientists emphasize the protection of their newest findings to ensure priority in publishing, patenting, or applications.&quot; ## [10] &quot;If you wish to comment about the questions in this survey, you may do so here. You are not required to comment.&quot; These variable names are too long to be useful. Let’s look at the source publication (Kardash &amp; Edwards, 2012) to see if we can simplify them in a useful way. So, the Appendix labels each question with a short phrase and notes whether the concept is a norm (N) or counter-norm (CN). Let’s rename the variables using the single words. new_names &lt;- c( &quot;Timestamp&quot;, &quot;Disinterestedness&quot;, &quot;Organized Skepticism&quot;, &quot;Particularism&quot;, &quot;Communality&quot;, &quot;Organized Dogmatism&quot;, &quot;Self-interestedness&quot;, &quot;Universalism&quot;, &quot;Solitariness&quot;, &quot;Comments&quot; ) # Make new data frame with long and short names for reference survey_qs &lt;- tibble(q_long = names(survey_01), q_short = new_names) # Swap out old (long) names for new (short) names names(survey_01) &lt;- new_names These data are ‘wide’, meaning that there are multiple variables for each respondent. The data will be easier to visualize and analyze if we make the data ‘longer’. survey_01_long &lt;- survey_01 %&gt;% tidyr::pivot_longer(., !c(&#39;Timestamp&#39;, &#39;Comments&#39;), names_to = &quot;norm_counternorm&quot;, values_to = &quot;rating&quot;) We should indicate whether these are norms, and label them “N”, or counternorms, and label them “CN”. survey_01_long &lt;- survey_01_long %&gt;% dplyr::mutate(., type = if_else( norm_counternorm %in% c( &quot;Disinterestedness&quot;, &quot;Organized Skepticism&quot;, &quot;Communality&quot;, &quot;Universalism&quot; ), &quot;Norm&quot;, &quot;Counternorm&quot; )) Finally, there is a “test” set of responses that I used to test this data processing workflow. We should delete those responses since they are nonsensical–I just hit random buttons to make some data. survey_01_long_clean &lt;- survey_01_long %&gt;% dplyr::filter(., is.na(Comments)) Visualize data What participants were asked Perceptions of scientific practices The following statements reflect different values about how the scientific research community should function, according to studies of scientists’ work. Use the scale below to indicate the extent to which you personally feel it ACTUALLY REPRESENTS the behavior of most scientists. Source: Kardash, C. M. &amp; Edwards, O. V. (2012). Thinking and behaving like scientists: Perceptions of undergraduate science interns and their faculty mentors. Instructional Science, 40(6), 875–899. https://doi.org/10.1007/s11251-011-9195-0. survey_qs %&gt;% kableExtra::kable(., format=&#39;html&#39;) q_long q_short Timestamp Timestamp Scientists are generally motivated by the desire for knowledge and discovery, and not by the possibility of personal gain. Disinterestedness Scientists make an attempt to consider all new evidence, hypotheses, theories, and innovations, even those that challenge or contradict their own work. Organized Skepticism Scientists generally assess new knowledge and its applications based on the reputation and past productivity of the individual or research group. Particularism Scientists openly share new findings with all colleagues. Communality Scientists generally invest their careers in promoting their own most important findings, theories, or innovations. Organized Dogmatism Scientists compete with others in the same field for funding and recognition of their achievements. Self-interestedness Scientists generally evaluate research only on its merit (i.e., according to accepted standards of the field). Universalism Scientists emphasize the protection of their newest findings to ensure priority in publishing, patenting, or applications. Solitariness If you wish to comment about the questions in this survey, you may do so here. You are not required to comment. Comments Unique respondents There appear to be \\(n=\\) 21 respondents as of 2023-03-02 09:00:27. Rating distribution Remember, the rating scale was from 1: “not at all” to 5: “a great deal”. survey_01_long %&gt;% ggplot() + aes(norm_counternorm, rating, fill = type) + geom_violin() + geom_point(position = position_jitter(width = .07, height = .07), alpha = 0.5) + coord_flip() + ggtitle(&quot;Ratings of scientists&#39; adherence to norms and counternorms&quot;) + theme(legend.position = &quot;bottom&quot;) + theme(legend.title = element_blank()) survey_01_long %&gt;% ggplot() + aes(type, rating, fill = type) + geom_violin() + geom_point(position = position_jitter(width = .07, height = .07), alpha = 0.5) survey_01 %&gt;% dplyr::mutate(., n_cum = seq_along(Timestamp)) %&gt;% dplyr::mutate(., Timestamp = lubridate::mdy_hms(survey_01$Timestamp)) %&gt;% ggplot() + aes(Timestamp, n_cum) + geom_point() + geom_line() References Kardash, C. M., &amp; Edwards, O. V. (2012). Thinking and behaving like scientists: Perceptions of undergraduate science interns and their faculty mentors. Instructional Science, 40(6), 875–899. https://doi.org/10.1007/s11251-011-9195-0 "],["references.html", "References", " References Anderson, M. S., Ronning, E. A., Devries, R., &amp; Martinson, B. C. (2010). Extending the mertonian norms: Scientists’ subscription to norms of research. The Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095 Artner, R., Verliefde, T., Steegen, S., Gomes, S., Traets, F., Tuerlinckx, F., &amp; Vanpaemel, W. (2021). The reproducibility of statistical results in psychological research: An investigation using unpublished raw data. Psychological Methods, 26(5), 527–546. https://doi.org/10.1037/met0000365 Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature News, 533(7604), 452. https://doi.org/10.1038/533452a Baker, T. (2023, February 17). Review found “falsified data” in stanford president’s research, colleagues allege. https://stanforddaily.com/2023/02/17/internal-review-found-falsified-data-in-stanford-presidents-alzheimers-research-colleagues-allege/. Retrieved from https://stanforddaily.com/2023/02/17/internal-review-found-falsified-data-in-stanford-presidents-alzheimers-research-colleagues-allege/ Bargh, J. A., Chen, M., &amp; Burrows, L. (1996). Automaticity of social behavior: Direct effects of trait construct and stereotype-activation on action. Journal of Personality and Social Psychology, 71(2), 230–244. https://doi.org/10.1037//0022-3514.71.2.230 Begley, C. G. (2013). Six red flags for suspect work. Nature, 497(7450), 433–434. https://doi.org/10.1038/497433a Begley, C. G., &amp; Ellis, L. M. (2012). Drug development: Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a Bhattacharjee, Y. (2013). The mind of a con man. The New York Times. Retrieved from https://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html Botvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., … Schonberg, T. (2020). Variability in the analysis of a single neuroimaging dataset by many teams. Nature, 582(7810), 84–88. https://doi.org/10.1038/s41586-020-2314-9 Brainerd, J., &amp; You, J. (2018). What a massive database of retracted papers reveals about science publishing’s “death penalty.” Science. https://doi.org/10.1126/science.aav8384 Brown, N. J. L., &amp; Heathers, J. A. J. (2017). The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology. Social Psychological and Personality Science, 8(4), 363–369. https://doi.org/10.1177/1948550616673876 Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T.-H., Huber, J., Johannesson, M., … Wu, H. (2018). Evaluating the replicability of social science experiments in nature and science between 2010 and 2015. Nature Human Behaviour, 1. https://doi.org/10.1038/s41562-018-0399-z Carlisle, J. B. (2017). Data fabrication and other reasons for non-random sampling in 5087 randomised, controlled trials in anaesthetic and general medical journals. Anaesthesia, 72(8), 944–952. https://doi.org/10.1111/anae.13938 Carlisle, J. B. (2018). Seeking and reporting apparent research misconduct: Errors and integrity - a reply. Anaesthesia, 73(1), 126–128. https://doi.org/10.1111/anae.14148 Carney, D. R., Cuddy, A. J. C., &amp; Yap, A. J. (2010). Power posing: Brief nonverbal displays affect neuroendocrine levels and risk tolerance. Psychological Science, 21(10), 1363–1368. https://doi.org/10.1177/0956797610383437 Carpenter, S. (2012). Harvard psychology researcher committed fraud, U.S. Investigation concludes. Science. https://doi.org/10.1126/article.26972 Collaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716 CrossFit. (2019, July). Dr. Glenn begley: Perverse incentives promote scientific laziness, exaggeration, and desperation. Youtube. Retrieved from https://www.youtube.com/watch?v=YJADzllTM9w Cuddy, A. (2012). Your body language may shape who you are. Retrieved from https://www.ted.com/talks/amy_cuddy_your_body_language_may_shape_who_you_are Dominus, S. (2017). When the revolution came for amy cuddy. The New York Times. Retrieved from https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html Doyen, S., Klein, O., Pichon, C.-L., &amp; Cleeremans, A. (2012). Behavioral priming: It’s all in the mind, but whose mind? PloS One, 7(1), e29081. https://doi.org/10.1371/journal.pone.0029081 Earp, B. D., Everett, J. A. C., Madva, E. N., &amp; Hamlin, J. K. (2014). Out, damned spot: Can the “macbeth effect” be replicated? Basic and Applied Social Psychology, 36(1), 91–98. https://doi.org/10.1080/01973533.2013.856792 Economist. (2023). There is a worrying amount of fraud in medical research. The Economist. Retrieved from https://www.economist.com/science-and-technology/2023/02/22/there-is-a-worrying-amount-of-fraud-in-medical-research Errington, T. M., Denis, A., Perfito, N., Iorns, E., &amp; Nosek, B. A. (2021). Challenges for assessing replicability in preclinical cancer biology. eLife, 10, e67995. https://doi.org/10.7554/eLife.67995 Errington, T. M., Mathur, M., Soderberg, C. K., Denis, A., Perfito, N., Iorns, E., &amp; Nosek, B. A. (2021). Investigating the replicability of preclinical cancer biology. eLife, 10, e71601. https://doi.org/10.7554/eLife.71601 Feynman, R. P. (1974). Cargo cult science. Retrieved from https://calteches.library.caltech.edu/51/2/CargoCult.htm Fidler, F., &amp; Wilcox, J. (2021). Reproducibility of scientific results. In E. N. Zalta (Ed.), The stanford encyclopedia of philosophy (Summer 2021). Metaphysics Research Lab, Stanford University. Retrieved from https://plato.stanford.edu/archives/sum2021/entries/scientific-reproducibility/ Franco, A., Malhotra, N., &amp; Simonovits, G. (2014). Social science. Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484 Freedman, L. P., Cockburn, I. M., &amp; Simcoe, T. S. (2015). The economics of reproducibility in preclinical research. PLoS Biology, 13(6), e1002165. https://doi.org/10.1371/journal.pbio.1002165 Goodman, S. N., Fanelli, D., &amp; Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12–341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 Harris, R. (2017). Rigor mortis: How sloppy science creates worthless cures, crushes hope, and wastes billions (1st edition). Basic Books. Hypothesis, B. A. S. (2021, February). 13. “Negative data” and the file drawer problem. Youtube. Retrieved from https://www.youtube.com/watch?v=9I1qR8PTr54 John, L. K., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953 Kardash, C. M., &amp; Edwards, O. V. (2012). Thinking and behaving like scientists: Perceptions of undergraduate science interns and their faculty mentors. Instructional Science, 40(6), 875–899. https://doi.org/10.1007/s11251-011-9195-0 Kharasch, E. D., &amp; Houle, T. T. (2018). Seeking and reporting apparent research misconduct: Errors and integrity. Anaesthesia, 73(1), 125–126. https://doi.org/10.1111/anae.14147 Kim, S. Y., &amp; Kim, Y. (2018). The ethos of science and its correlates: An empirical analysis of scientists’ endorsement of mertonian norms. Science, Technology and Society, 23(1), 1–24. https://doi.org/10.1177/0971721817744438 Levelt, W. J. M., Drenth, P. J. D., &amp; Noort, E. (2012). Flawed science: The fraudulent research practices of social psychologist diederik stapel. https://pure.mpg.de/rest/items/item_1569964/component/file_1569966/content; pure.mpg.de. Retrieved from https://pure.mpg.de/rest/items/item_1569964/component/file_1569966/content Macfarlane, B., &amp; Cheng, M. (2008). Communism, universalism and disinterestedness: Re-examining contemporary support among academics for merton’s scientific norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y Merton, R. W. (1973). The normative structure of science. In R. K. Merton &amp; N. W. Storer (Eds.), The sociology of science: Theoretical and empirical investigations (pp. 267–278). The University of Chicago Press. Mitroff, I. I. (1974). Norms and counter-norms in a select group of the apollo moon scientists: A case study of the ambivalence of scientists. American Sociological Review, 39(4), 579–595. https://doi.org/10.2307/2094423 Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Sert, N. P. du, … Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. https://doi.org/10.1038/s41562-016-0021 Ngiam, W. (2020, April). ReproducibiliTea | simmons, nelson and simonsohn (2011). False-Positive psychology. Youtube. Retrieved from https://www.youtube.com/watch?v=bf3GqyBRgzY Nosek, B. A., &amp; Bar-Anan, Y. (2012). Scientific utopia i: Opening scientific communication. Psychological Inquiry, 23(3), 217–243. https://doi.org/10.1080/1047840X.2012.692215 Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., … Vazire, S. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73(2022), 719–748. https://doi.org/10.1146/annurev-psych-020821-114157 Nuijten, M. B., Hartgerink, C. H. J., Assen, M. A. L. M. van, Epskamp, S., &amp; Wicherts, J. M. (2015). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 1–22. https://doi.org/10.3758/s13428-015-0664-2 NYU Health Sciences Library. (2013, November). Data sharing and management snafu in 3 short acts (higher quality). Youtube. Retrieved from https://www.youtube.com/watch?v=66oNv_DJuPc Oreskes, N. (2019). Why trust science. Princeton University Press. Peng, R. D., &amp; Hicks, S. C. (2021). Reproducible research: A retrospective. Annual Review of Public Health, 42, 79–93. https://doi.org/10.1146/annurev-publhealth-012420-105110 Prinz, F., Schlange, T., &amp; Asadullah, K. (2011). Believe it or not: How much can we rely on published data on potential drug targets? Nature Reviews. Drug Discovery, 10(9), 712. https://doi.org/10.1038/nrd3439-c1 Ranehill, E., Dreber, A., Johannesson, M., Leiberg, S., Sul, S., &amp; Weber, R. A. (2015). Assessing the robustness of power posing: No effect on hormones and risk tolerance in a large sample of men and women. Psychological Science, 26(5), 653–656. https://doi.org/10.1177/0956797614553946 Ritchie, S. (2020). Science fictions: Exposing fraud, bias, negligence and hype in science (1st ed.). Penguin Random House. Retrieved from https://www.amazon.com/Science-Fictions/dp/1847925669 rjlipton. (2023, January). Cargo cult redo. https://rjlipton.wpcomstaging.com/2023/01/06/cargo-cult-redo/. Retrieved from https://rjlipton.wpcomstaging.com/2023/01/06/cargo-cult-redo/ Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638 Sagan, C. (1996). The demon-haunted world: Science as a candle in the dark (pp. 200–218). Ballantine Books. Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., … Nosek, B. A. (2018). Many analysts, one data set: Making transparent how variations in analytic choices affect results. Advances in Methods and Practices in Psychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646 Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 Szucs, D., &amp; Ioannidis, J. P. A. (2017). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. PLoS Biology, 15(3), e2000797. https://doi.org/10.1371/journal.pbio.2000797 University of California Television (UCTV). (2018, May). Improving openness and innovation in scholarly communication with brian nosek. Youtube. Retrieved from https://www.youtube.com/watch?v=YEwqohAjfZc Whitt, C. M., Miranda, J. F., &amp; Tullett, A. M. (2022). History of replication failures in psychology. In W. O’Donohue, A. Masuda, &amp; S. Lilienfeld (Eds.), Avoiding questionable research practices in applied psychology (pp. 73–97). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-04968-2\\_4 Youyou, W., Yang, Y., &amp; Uzzi, B. (2023). A discipline-wide investigation of the replicability of psychology papers over the past two decades. Proceedings of the National Academy of Sciences of the United States of America, 120(6), e2208863120. https://doi.org/10.1073/pnas.2208863120 Zhong, C.-B., &amp; Liljenquist, K. (2006). Washing away your sins: Threatened morality and physical cleansing. Science, 313(5792), 1451–1452. https://doi.org/10.1126/science.1130726 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
