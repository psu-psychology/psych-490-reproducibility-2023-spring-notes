# Meta-analysis and many-analysts {-}

## Roadmap {-}

- Meta-analysis & many analysts
    - [@Wilson2014-ol](https://www.psychologicalscience.org/observer/introduction-to-meta-analysis-a-guide-for-the-novice)
    - [@Silberzahn2018-st](https://doi.org/10.1177/2515245917747646)
- Reminder
  - [Exercise 04: Replication](https://psu-psychology.github.io/psych-490-reproducibility-2023-spring/ex_replication.html) [distributed]{.green_assigned}. [Due Thursday, March 30]{.due_orange}.
    
### Resources for final projects {-}

McManus, K. (2022). Are replication studies infrequent because of negative attitudes?: Insights from a survey of attitudes and practices in second language research. *Studies in Second Language Acquisition*, 44(5), 1410–1423. https://doi.org/10.1017/S0272263121000838.

>1) While this is technically also based on Retraction Watch, several reference managers (definitely [Zotero](https://www.zotero.org/) and [Endnote](https://guides.libraries.psu.edu/c.php?g=417962&p=2876102), maybe also [Papers/Readcube](https://www.papersapp.com/)) will automatically flag retracted items in your library now. Judging by social media reactions, that's a pretty effective way to alert students and researchers: certainly more so than anything that requires actively looking for the information. Might be fun to have students, or at least those using one of those tools anyway, test how this looks by importing a retracted study.

Edlund, J. E., Okdie, B. M. & Scherer, C. R. (2022). Best practices for considering retractions. *Current Psychology*. https://doi.org/10.1007/s12144-022-03764-x

### Data sharing and COVID{-}

- [@Wu2023-od](https://www.theatlantic.com/science/archive/2023/03/covid-pandemic-origins-missing-evidence-debate/673460/)
- Article in The Atlantic about a dispute over shared data--who can use it, when, and for what purposes--that could shed light on the origins of the pandemic.

## Meta-analysis {-}

- Multiple studies, ideally published and unpublished (why?)
- What is the distribution of effect sizes?

### [@Van_Agteren2021-al](http://dx.doi.org/10.1038/s41562-021-01093-w) {-}

van Agteren, J., Iasiello, M., Lo, L., Bartholomaeus, J., Kopsaftis, Z., Carey, M. & Kyrios, M. (2021). A systematic review and meta-analysis of psychological interventions to improve mental wellbeing. *Nature Human Behaviour*. https://doi.org/10.1038/s41562-021-01093-w.

>Our current understanding of the efficacy of psychological interventions in improving mental states of wellbeing is incomplete. This study aimed to overcome limitations of previous reviews by examining the efficacy of distinct types of psychological interventions, irrespective of their theoretical underpinning, and the impact of various moderators, in a unified systematic review and meta-analysis. Four-hundred-and-nineteen randomized controlled trials from clinical and non-clinical populations (n = 53,288) were identified for inclusion. Mindfulness-based and multi-component positive psychological interventions demonstrated the greatest efficacy in both clinical and non-clinical populations. Meta-analyses also found that singular positive psychological interventions, cognitive and behavioural therapy-based, acceptance and commitment therapy-based, and reminiscence interventions were impactful. Effect sizes were moderate at best, but differed according to target population and moderator, most notably intervention intensity. The evidence quality was generally low to moderate. While the evidence requires further advancement, the review provides insight into how psychological interventions can be designed to improve mental wellbeing.
>
>-- [@Van_Agteren2021-al](http://dx.doi.org/10.1038/s41562-021-01093-w)

```{r, out.width="100%", fig.cap="Figure 2 from [@Van_Agteren2021-al](http://dx.doi.org/10.1038/s41562-021-01093-w)"}
knitr::include_graphics("include/img/vanagteren-2021-fig-02.png")
```

```{r, out.width="100%", fig.cap="Figure 3 from [@Van_Agteren2021-al](http://dx.doi.org/10.1038/s41562-021-01093-w)"}
knitr::include_graphics("include/img/vanagteren-2021-fig-03.png")
```

## [@Silberzahn2018-st](https://doi.org/10.1177/2515245917747646) {-}

>Abstract
>
>Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.
>
>-- [@Silberzahn2018-st](https://doi.org/10.1177/2515245917747646)

```{r, out.width="100%", fig.cap="Figure 2 from [@Silberzahn2018-st](https://doi.org/10.1177/2515245917747646); Point estimates (in order of magnitude) and 95% confidence intervals for the effect of soccer players’ skin tone on the number of red cards awarded by referees. Reported results, along with the analytic approach taken, are shown for each of the 29 analytic teams. The teams are ordered so that the smallest reported effect size is at the top and the largest is at the bottom. The asterisks indicate upper bounds that have been truncated to increase the interpretability of the plot; the actual upper bounds of the confidence intervals were 11.47 for Team 21 and 78.66 for Team 27. OLS = ordinary least squares; WLS = weighted least squares."}
knitr::include_graphics("include/img/silberzahn-2018-fig-02.jpg")
```

```{r, out.width="100%", fig.cap="Figure 3 from [@Silberzahn2018-st](https://doi.org/10.1177/2515245917747646); Point estimates (clustered by analytic approach) and 95% confidence intervals for the effect of soccer players’ skin tone on the number of red cards awarded by referees. Reported results, along with the analytic approach taken, are shown for each of the 29 analytic teams. The teams are clustered according to the distribution used in their analyses; within each cluster, the teams are listed in order of the magnitude of the reported effect size, from smallest at the top to largest at the bottom. The asterisks indicate upper bounds that have been truncated to increase the interpretability of the plot (see Fig. 2). OLS = ordinary least squares; WLS = weighted least squares; Misc = miscellaneous."}
knitr::include_graphics("include/img/silberzahn-2018-fig-03.jpg")
```

::: {.infobox}

Odds ratios (OR) [@Szumilas2010-uj](https://www.ncbi.nlm.nih.gov/pubmed/20842279) {-}

- OR < 1: Outcome *less* likely than comparison
- OR = 1: Outcome and comparison equally likely
- OR > 1: Outcome *more* likely than comparison

:::

## Next time... {-}

- Final project work day
